{
  "method": "Linear Discriminant Analysis",
  "summary": "Linear discriminant analysis (LDA is a generalization of Fisher's linear discriminant, a method used in statistics, pattern recognition and machine learning to find a linear combination of features that characterizes or separates two or more classes of objects or events. ",
  "desc": "<span class='help-method'>Linear Discriminant Analysis a classifier with a linear decision boundary, generated by fitting class conditional densities to the data and using Bayes’ rule. The model fits a Gaussian density to each class, assuming that all classes share the same covariance matrix. The fitted model can also be used to reduce the dimensionality of the input by projecting it to the most discriminative directions. New in version 0.17: LinearDiscriminantAnalysis.",
  "urlparagraph": "Read more in the User Guide.",
  "url": "http://scikit-learn.org/stable/modules/generated/sklearn.discriminant_analysis.LinearDiscriminantAnalysis.html#sklearn.discriminant_analysis.LinearDiscriminantAnalysis",
  "tutorial": [{
    "desc": "Learn more on Oncoscape + Sci-Kit clustering methods",
    "url": "https://www.youtube.com/embed/XQu8TTBmGhA"
  }],
  "params": [{
      "name": "solver",
      "type": "string, optional Solver to use, possible values",
      "desc": "'svd': Singular value decomposition (default). Does not compute the covariance matrix, therefore this solver is recommended for data with a large number of features. ‘lsqr’: Least squares solution, can be combined with shrinkage. ‘eigen’: Eigenvalue decomposition, can be combined with shrinkage."
    },
    {
      "name": "shrinkage",
      "type": "string or float, optional",
      "desc": "Shrinkage parameter, possible values: None: no shrinkage (default). ‘auto’: automatic shrinkage using the Ledoit-Wolf lemma. float between 0 and 1: fixed shrinkage parameter."
    },
    {
      "name": "priors",
      "type": "array, optional, shape (n_classes,)",
      "desc": "Class priors."
    },
    {
      "name": "n_components",
      "type": "int, optional",
      "desc": "Number of components (< n_classes - 1) for dimensionality reduction."
    },
    {
      "name": "store_covariance",
      "type": "bool, optional",
      "desc": "Additionally compute class covariance matrix (default False), used only in ‘svd’ solver."
    },
    {
      "name": "tol",
      "type": "float, optional, (default 1.0e-4)",
      "desc": "Threshold used for rank estimation in SVD solver."
    }
  ],
  "attrs": [{
      "name": "coef",
      "type": "array, shape (n_features,) or (n_classes, n_features) Weight vector(s). intercept",
      "desc": ""
    },
    {
      "name": "intercept",
      "type": "array, shape (n_features,)",
      "desc": "Intercept term."
    },
    {
      "name": "covariance",
      "type": "array-like, shape (n_features, n_features)",
      "desc": "Covariance matrix (shared by all classes)."
    },
    {
      "name": "explained_variance_ratio",
      "type": "array, shape (n_components,)",
      "desc": "Percentage of variance explained by each of the selected components. If n_components is not set then all components are stored and the sum of explained variances is equal to 1.0. Only available when eigen or svd solver is used."
    },
    {
      "name": "means",
      "type": "array-like, shape (n_classes, n_features)",
      "desc": "Class means."
    },
    {
      "name": "priors",
      "type": "array-like, shape (n_classes,)",
      "desc": "Class priors (sum to 1)."
    },
    {
      "name": "scalings",
      "type": "array-like, shape (rank, n_classes - 1)",
      "desc": "Scaling of the features in the space spanned by the class centroids."
    },
    {
      "name": "xbar",
      "type": "array-like, shape (n_features,)",
      "desc": "Overall mean."
    },
    {
      "name": "classes",
      "type": "array-like, shape (n_classes,)",
      "desc": "Unique class labels."
    }
  ],
  "citations": [{
      "name": "The Elements of Statistical Learning",
      "desc": "(1, 2) “The Elements of Statistical Learning”, Hastie T., Tibshirani R., Friedman J., Section 4.3, p.106-119, 2008.",
      "url": "https://web.stanford.edu/~hastie/Papers/ESLII.pdf"
    },
    {
      "name": "Honey, I Shrunk the Sample Covariance Matrix.",
      "desc": "Ledoit O, Wolf M. Honey, I Shrunk the Sample Covariance Matrix. The Journal of Portfolio Management 30(4), 110-119, 2004.",
      "url": "http://www.ledoit.net/honey.pdf"
    }
  ]
}
