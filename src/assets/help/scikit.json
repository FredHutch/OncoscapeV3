[{"method":"Dictionary Learning","desc":"Dictionary learning<br />Finds a dictionary (a set of atoms) that can best be used to represent data<br />using a sparse code.<br />Solves the optimization problem:<br />(U^*,V^*) = argmin 0.5 || Y - U V ||_2^2 + alpha * || U ||_1<br />(U,V)<br />with || V_k ||_2 = 1 for all  0 <= k < n_components<br />Read more in the User Guide.<br />","url":"http://scikit-learn.org/stable/modules/generated/sklearn.decomposition.DictionaryLearning.html#sklearn.decomposition.DictionaryLearning","params":[{"name":"n_components","type":"int,","desc":"number of dictionary elements to extract"},{"name":"alpha","type":"float,","desc":"sparsity controlling parameter"},{"name":"max_iter","type":"int,","desc":"maximum number of iterations to perform"},{"name":"tol","type":"float,","desc":"tolerance for numerical error"},{"name":"fit_algorithm","type":"{‘lars’, ‘cd’}","desc":"lars: uses the least angle regression method to solve the lasso problem (linear_model.lars_path) cd: uses the coordinate descent method to compute the Lasso solution (linear_model.Lasso). Lars will be faster if the estimated components are sparse.  New in version 0.17: cd coordinate descent method to improve speed."},{"name":"transform_algorithm","type":"{‘lasso_lars’, ‘lasso_cd’, ‘lars’, ‘omp’,     ‘threshold’}","desc":"Algorithm used to transform the data lars: uses the least angle regression method (linear_model.lars_path) lasso_lars: uses Lars to compute the Lasso solution lasso_cd: uses the coordinate descent method to compute the Lasso solution (linear_model.Lasso). lasso_lars will be faster if the estimated components are sparse. omp: uses orthogonal matching pursuit to estimate the sparse solution threshold: squashes to zero all coefficients less than alpha from the projection dictionary * X'  New in version 0.17: lasso_cd coordinate descent method to improve speed."},{"name":"transform_n_nonzero_coefs","type":"int, 0.1 * n_features by default","desc":"Number of nonzero coefficients to target in each column of the solution. This is only used by algorithm=’lars’ and algorithm=’omp’ and is overridden by alpha in the omp case."},{"name":"transform_alpha","type":"float, 1. by default","desc":"If algorithm=’lasso_lars’ or algorithm=’lasso_cd’, alpha is the penalty applied to the L1 norm. If algorithm=’threshold’, alpha is the absolute value of the threshold below which coefficients will be squashed to zero. If algorithm=’omp’, alpha is the tolerance parameter: the value of the reconstruction error targeted. In this case, it overrides n_nonzero_coefs."},{"name":"n_jobs","type":"int,","desc":"number of parallel jobs to run"},{"name":"code_init","type":"array of shape (n_samples, n_components),","desc":"initial value for the code, for warm restart"},{"name":"dict_init","type":"array of shape (n_components, n_features),","desc":"initial values for the dictionary, for warm restart"},{"name":"verbose","type":"bool, optional (default","desc":"To control the verbosity of the procedure."},{"name":"split_sign","type":"bool, False by default","desc":"Whether to split the sparse feature vector into the concatenation of its negative part and its positive part. This can improve the performance of downstream classifiers."},{"name":"random_state","type":"int, RandomState instance or None, optional (default=None)","desc":"If int, random_state is the seed used by the random number generator; If RandomState instance, random_state is the random number generator; If None, the random number generator is the RandomState instance used by np.random."}],"attrs":[{"name":"components_","type":"array, [n_components, n_features]","desc":"dictionary atoms extracted from the data"},{"name":"error_","type":"array","desc":"vector of errors at each iteration"},{"name":"n_iter_","type":"int","desc":"Number of iterations run."}]},{"method":"Factor Analysis","desc":"Factor Analysis (FA)<br />A simple linear generative model with Gaussian latent variables.<br />The observations are assumed to be caused by a linear transformation of<br />lower dimensional latent factors and added Gaussian noise.<br />Without loss of generality the factors are distributed according to a<br />Gaussian with zero mean and unit covariance. The noise is also zero mean<br />and has an arbitrary diagonal covariance matrix.<br />If we would restrict the model further, by assuming that the Gaussian<br />noise is even isotropic (all diagonal entries are the same) we would obtain<br />PPCA.<br />FactorAnalysis performs a maximum likelihood estimate of the so-called<br />loading matrix, the transformation of the latent variables to the<br />observed ones, using expectation-maximization (EM).<br />Read more in the User Guide.<br />","url":"http://scikit-learn.org/stable/modules/generated/sklearn.decomposition.FactorAnalysis.html#sklearn.decomposition.FactorAnalysis","params":[{"name":"n_components","type":"int | None","desc":"Dimensionality of latent space, the number of components of X that are obtained after transform. If None, n_components is set to the number of features."},{"name":"tol","type":"float","desc":"Stopping tolerance for EM algorithm."},{"name":"copy","type":"bool","desc":"Whether to make a copy of X. If False, the input X gets overwritten during fitting."},{"name":"max_iter","type":"int","desc":"Maximum number of iterations."},{"name":"noise_variance_init","type":"None | array, shape=(n_features,)","desc":"The initial guess of the noise variance for each feature. If None, it defaults to np.ones(n_features)"},{"name":"svd_method","type":"{‘lapack’, ‘randomized’}","desc":"Which SVD method to use. If ‘lapack’ use standard SVD from scipy.linalg, if ‘randomized’ use fast randomized_svd function. Defaults to ‘randomized’. For most applications ‘randomized’ will be sufficiently precise while providing significant speed gains. Accuracy can also be improved by setting higher values for iterated_power. If this is not sufficient, for maximum precision you should choose ‘lapack’."},{"name":"iterated_power","type":"int, optional","desc":"Number of iterations for the power method. 3 by default. Only used if svd_method equals ‘randomized’"},{"name":"random_state","type":"int, RandomState instance or None, optional (default=0)","desc":"If int, random_state is the seed used by the random number generator; If RandomState instance, random_state is the random number generator; If None, the random number generator is the RandomState instance used by np.random. Only used when svd_method equals ‘randomized’."}],"attrs":[{"name":"components_","type":"array, [n_components, n_features]","desc":"Components with maximum variance."},{"name":"loglike_","type":"list, [n_iterations]","desc":"The log likelihood at each iteration."},{"name":"noise_variance_","type":"array, shape=(n_features,)","desc":"The estimated noise variance for each feature."},{"name":"n_iter_","type":"int","desc":"Number of iterations run."}]},{"method":"Fast ICA","desc":"FastICA: a fast algorithm for Independent Component Analysis.<br />Read more in the User Guide.<br />","url":"http://scikit-learn.org/stable/modules/generated/sklearn.decomposition.FastICA.html#sklearn.decomposition.FastICA","params":[{"name":"n_components","type":"int, optional","desc":"Number of components to use. If none is passed, all are used."},{"name":"algorithm","type":"{‘parallel’, ‘deflation’}","desc":"Apply parallel or deflational algorithm for FastICA."},{"name":"whiten","type":"boolean, optional","desc":"If whiten is false, the data is already considered to be whitened, and no whitening is performed."},{"name":"fun","type":"string or function, optional. Default","desc":"The functional form of the G function used in the approximation to neg-entropy. Could be either ‘logcosh’, ‘exp’, or ‘cube’. You can also provide your own function. It should return a tuple containing the value of the function, and of its derivative, in the point. Example:  def my_g(x): return x ** 3, 3 * x ** 2"},{"name":"fun_args","type":"dictionary, optional","desc":"Arguments to send to the functional form. If empty and if fun=’logcosh’, fun_args will take value {‘alpha’ : 1.0}."},{"name":"max_iter","type":"int, optional","desc":"Maximum number of iterations during fit."},{"name":"tol","type":"float, optional","desc":"Tolerance on update at each iteration."},{"name":"w_init","type":"None of an (n_components, n_components) ndarray","desc":"The mixing matrix to be used to initialize the algorithm."},{"name":"random_state","type":"int, RandomState instance or None, optional (default=None)","desc":"If int, random_state is the seed used by the random number generator; If RandomState instance, random_state is the random number generator; If None, the random number generator is the RandomState instance used by np.random."}],"attrs":[{"name":"components_","type":"2D array, shape (n_components, n_features)","desc":"The unmixing matrix."},{"name":"mixing_","type":"array, shape (n_features, n_components)","desc":"The mixing matrix."},{"name":"n_iter_","type":"int","desc":"If the algorithm is “deflation”, n_iter is the maximum number of iterations run across all components. Else they are just the number of iterations taken to converge."}]},{"method":"Incremental PCA","desc":"Incremental principal components analysis (IPCA).<br />Linear dimensionality reduction using Singular Value Decomposition of<br />centered data, keeping only the most significant singular vectors to<br />project the data to a lower dimensional space.<br />Depending on the size of the input data, this algorithm can be much more<br />memory efficient than a PCA.<br />This algorithm has constant memory complexity, on the order<br />of batch_size, enabling use of np.memmap files without loading the<br />entire file into memory.<br />The computational overhead of each SVD is<br />O(batch_size * n_features ** 2), but only 2 * batch_size samples<br />remain in memory at a time. There will be n_samples / batch_size SVD<br />computations to get the principal components, versus 1 large SVD of<br />complexity O(n_samples * n_features ** 2) for PCA.<br />Read more in the User Guide.<br />","url":"http://scikit-learn.org/stable/modules/generated/sklearn.decomposition.IncrementalPCA.html#sklearn.decomposition.IncrementalPCA","params":[{"name":"n_components","type":"int or None, (default=None)","desc":"Number of components to keep. If n_components `` is ``None, then n_components is set to min(n_samples, n_features)."},{"name":"whiten","type":"bool, optional","desc":"When True (False by default) the components_ vectors are divided by n_samples times components_ to ensure uncorrelated outputs with unit component-wise variances. Whitening will remove some information from the transformed signal (the relative variance scales of the components) but can sometimes improve the predictive accuracy of the downstream estimators by making data respect some hard-wired assumptions."},{"name":"copy","type":"bool, (default=True)","desc":"If False, X will be overwritten. copy=False can be used to save memory but is unsafe for general use."},{"name":"batch_size","type":"int or None, (default=None)","desc":"The number of samples to use for each batch. Only used when calling fit. If batch_size is None, then batch_size is inferred from the data and set to 5 * n_features, to provide a balance between approximation accuracy and memory consumption."}],"attrs":[{"name":"components_","type":"array, shape (n_components, n_features)","desc":"Components with maximum variance."},{"name":"explained_variance_","type":"array, shape (n_components,)","desc":"Variance explained by each of the selected components."},{"name":"explained_variance_ratio_","type":"array, shape (n_components,)","desc":"Percentage of variance explained by each of the selected components. If all components are stored, the sum of explained variances is equal to 1.0."},{"name":"singular_values_","type":"array, shape (n_components,)","desc":"The singular values corresponding to each of the selected components. The singular values are equal to the 2-norms of the n_components variables in the lower-dimensional space."},{"name":"mean_","type":"array, shape (n_features,)","desc":"Per-feature empirical mean, aggregate over calls to partial_fit."},{"name":"var_","type":"array, shape (n_features,)","desc":"Per-feature empirical variance, aggregate over calls to partial_fit."},{"name":"noise_variance_","type":"float","desc":"The estimated noise covariance following the Probabilistic PCA model from Tipping and Bishop 1999. See “Pattern Recognition and Machine Learning” by C. Bishop, 12.2.1 p. 574 or http://www.miketipping.com/papers/met-mppca.pdf."},{"name":"n_components_","type":"int","desc":"The estimated number of components. Relevant when n_components=None."},{"name":"n_samples_seen_","type":"int","desc":"The number of samples processed by the estimator. Will be reset on new calls to fit, but increments across partial_fit calls."}]},{"method":"Kernel PCA","desc":"Kernel Principal component analysis (KPCA)<br />Non-linear dimensionality reduction through the use of kernels (see<br />Pairwise metrics, Affinities and Kernels).<br />Read more in the User Guide.<br />","url":"http://scikit-learn.org/stable/modules/generated/sklearn.decomposition.KernelPCA.html#sklearn.decomposition.KernelPCA","params":[{"name":"n_components","type":"int, default=None","desc":"Number of components. If None, all non-zero components are kept."},{"name":"kernel","type":"“linear” | “poly” | “rbf” | “sigmoid” | “cosine” | “precomputed”","desc":"Kernel. Default=”linear”."},{"name":"gamma","type":"float, default=1/n_features","desc":"Kernel coefficient for rbf, poly and sigmoid kernels. Ignored by other kernels."},{"name":"degree","type":"int, default=3","desc":"Degree for poly kernels. Ignored by other kernels."},{"name":"coef0","type":"float, default=1","desc":"Independent term in poly and sigmoid kernels. Ignored by other kernels."},{"name":"kernel_params","type":"mapping of string to any, default=None","desc":"Parameters (keyword arguments) and values for kernel passed as callable object. Ignored by other kernels."},{"name":"alpha","type":"int, default=1.0","desc":"Hyperparameter of the ridge regression that learns the inverse transform (when fit_inverse_transform=True)."},{"name":"fit_inverse_transform","type":"bool, default=False","desc":"Learn the inverse transform for non-precomputed kernels. (i.e. learn to find the pre-image of a point)"},{"name":"eigen_solver","type":"string [‘auto’|’dense’|’arpack’], default=’auto’","desc":"Select eigensolver to use. If n_components is much less than the number of training samples, arpack may be more efficient than the dense eigensolver."},{"name":"tol","type":"float, default=0","desc":"Convergence tolerance for arpack. If 0, optimal value will be chosen by arpack."},{"name":"max_iter","type":"int, default=None","desc":"Maximum number of iterations for arpack. If None, optimal value will be chosen by arpack."},{"name":"remove_zero_eig","type":"boolean, default=False","desc":"If True, then all components with zero eigenvalues are removed, so that the number of components in the output may be < n_components (and sometimes even zero due to numerical instability). When n_components is None, this parameter is ignored and components with zero eigenvalues are removed regardless."},{"name":"random_state","type":"int, RandomState instance or None, optional (default=None)","desc":"If int, random_state is the seed used by the random number generator; If RandomState instance, random_state is the random number generator; If None, the random number generator is the RandomState instance used by np.random. Used when eigen_solver == ‘arpack’.  New in version 0.18."},{"name":"copy_X","type":"boolean, default=True","desc":"If True, input X is copied and stored by the model in the X_fit_ attribute. If no further changes will be done to X, setting copy_X=False saves memory by storing a reference.  New in version 0.18."},{"name":"n_jobs","type":"int, default=1","desc":"The number of parallel jobs to run. If -1, then the number of jobs is set to the number of CPU cores.  New in version 0.18."}],"attrs":[{"name":"lambdas_","type":"array, (n_components,)","desc":"Eigenvalues of the centered kernel matrix in decreasing order. If n_components and remove_zero_eig are not set, then all values are stored."},{"name":"alphas_","type":"array, (n_samples, n_components)","desc":"Eigenvectors of the centered kernel matrix. If n_components and remove_zero_eig are not set, then all components are stored."},{"name":"dual_coef_","type":"array, (n_samples, n_features)","desc":"Inverse transform matrix. Set if fit_inverse_transform is True."},{"name":"X_transformed_fit_","type":"array, (n_samples, n_components)","desc":"Projection of the fitted data on the kernel principal components."},{"name":"X_fit_","type":"(n_samples, n_features)","desc":"The data used to fit the model. If copy_X=False, then X_fit_ is a reference. This attribute is used for the calls to transform."}]},{"method":"Latent Dirichlet Allocation","desc":"Latent Dirichlet Allocation with online variational Bayes algorithm<br />New in version 0.17.<br />Read more in the User Guide.<br />","url":"http://scikit-learn.org/stable/modules/generated/sklearn.decomposition.LatentDirichletAllocation.html#sklearn.decomposition.LatentDirichletAllocation","params":[{"name":"n_components","type":"int, optional (default=10)","desc":"Number of topics."},{"name":"doc_topic_prior","type":"float, optional (default=None)","desc":"Prior of document topic distribution theta. If the value is None, defaults to 1 / n_components. In the literature, this is called alpha."},{"name":"topic_word_prior","type":"float, optional (default=None)","desc":"Prior of topic word distribution beta. If the value is None, defaults to 1 / n_components. In the literature, this is called eta."},{"name":"learning_method","type":"‘batch’ | ‘online’, default=’online’","desc":"Method used to update _component. Only used in fit method. In general, if the data size is large, the online update will be much faster than the batch update. The default learning method is going to be changed to ‘batch’ in the 0.20 release. Valid options: 'batch': Batch variational Bayes method. Use all training data in     each EM update.     Old `components_` will be overwritten in each iteration. 'online': Online variational Bayes method. In each EM update, use     mini-batch of training data to update the ``components_``     variable incrementally. The learning rate is controlled by the     ``learning_decay`` and the ``learning_offset`` parameters."},{"name":"learning_decay","type":"float, optional (default=0.7)","desc":"It is a parameter that control learning rate in the online learning method. The value should be set between (0.5, 1.0] to guarantee asymptotic convergence. When the value is 0.0 and batch_size is n_samples, the update method is same as batch learning. In the literature, this is called kappa."},{"name":"learning_offset","type":"float, optional (default=10.)","desc":"A (positive) parameter that downweights early iterations in online learning.  It should be greater than 1.0. In the literature, this is called tau_0."},{"name":"max_iter","type":"integer, optional (default=10)","desc":"The maximum number of iterations."},{"name":"batch_size","type":"int, optional (default=128)","desc":"Number of documents to use in each EM iteration. Only used in online learning."},{"name":"evaluate_every","type":"int, optional (default=0)","desc":"How often to evaluate perplexity. Only used in fit method. set it to 0 or negative number to not evalute perplexity in training at all. Evaluating perplexity can help you check convergence in training process, but it will also increase total training time. Evaluating perplexity in every iteration might increase training time up to two-fold."},{"name":"total_samples","type":"int, optional (default=1e6)","desc":"Total number of documents. Only used in the partial_fit method."},{"name":"perp_tol","type":"float, optional (default=1e-1)","desc":"Perplexity tolerance in batch learning. Only used when evaluate_every is greater than 0."},{"name":"mean_change_tol","type":"float, optional (default=1e-3)","desc":"Stopping tolerance for updating document topic distribution in E-step."},{"name":"max_doc_update_iter","type":"int (default=100)","desc":"Max number of iterations for updating document topic distribution in the E-step."},{"name":"n_jobs","type":"int, optional (default=1)","desc":"The number of jobs to use in the E-step. If -1, all CPUs are used. For n_jobs below -1, (n_cpus + 1 + n_jobs) are used."},{"name":"verbose","type":"int, optional (default=0)","desc":"Verbosity level."},{"name":"random_state","type":"int, RandomState instance or None, optional (default=None)","desc":"If int, random_state is the seed used by the random number generator; If RandomState instance, random_state is the random number generator; If None, the random number generator is the RandomState instance used by np.random."},{"name":"n_topics","type":"int, optional (default=None)","desc":"This parameter has been renamed to n_components and will be removed in version 0.21. .. deprecated:: 0.19"}],"attrs":[{"name":"components_","type":"array, [n_components, n_features]","desc":"Variational parameters for topic word distribution. Since the complete conditional for topic word distribution is a Dirichlet, components_[i, j] can be viewed as pseudocount that represents the number of times word j was assigned to topic i. It can also be viewed as distribution over the words for each topic after normalization: model.components_ / model.components_.sum(axis=1)[:, np.newaxis]."},{"name":"n_batch_iter_","type":"int","desc":"Number of iterations of the EM step."},{"name":"n_iter_","type":"int","desc":"Number of passes over the dataset."}]},{"method":"Mini Batch Dictionary Learning","desc":"Mini-batch dictionary learning<br />Finds a dictionary (a set of atoms) that can best be used to represent data<br />using a sparse code.<br />Solves the optimization problem:<br />(U^*,V^*) = argmin 0.5 || Y - U V ||_2^2 + alpha * || U ||_1<br />(U,V)<br />with || V_k ||_2 = 1 for all  0 <= k < n_components<br />Read more in the User Guide.<br />","url":"http://scikit-learn.org/stable/modules/generated/sklearn.decomposition.MiniBatchDictionaryLearning.html#sklearn.decomposition.MiniBatchDictionaryLearning","params":[{"name":"n_components","type":"int,","desc":"number of dictionary elements to extract"},{"name":"alpha","type":"float,","desc":"sparsity controlling parameter"},{"name":"n_iter","type":"int,","desc":"total number of iterations to perform"},{"name":"fit_algorithm","type":"{‘lars’, ‘cd’}","desc":"lars: uses the least angle regression method to solve the lasso problem (linear_model.lars_path) cd: uses the coordinate descent method to compute the Lasso solution (linear_model.Lasso). Lars will be faster if the estimated components are sparse."},{"name":"n_jobs","type":"int,","desc":"number of parallel jobs to run"},{"name":"batch_size","type":"int,","desc":"number of samples in each mini-batch"},{"name":"shuffle","type":"bool,","desc":"whether to shuffle the samples before forming batches"},{"name":"dict_init","type":"array of shape (n_components, n_features),","desc":"initial value of the dictionary for warm restart scenarios"},{"name":"transform_algorithm","type":"{‘lasso_lars’, ‘lasso_cd’, ‘lars’, ‘omp’,     ‘threshold’}","desc":"Algorithm used to transform the data. lars: uses the least angle regression method (linear_model.lars_path) lasso_lars: uses Lars to compute the Lasso solution lasso_cd: uses the coordinate descent method to compute the Lasso solution (linear_model.Lasso). lasso_lars will be faster if the estimated components are sparse. omp: uses orthogonal matching pursuit to estimate the sparse solution threshold: squashes to zero all coefficients less than alpha from the projection dictionary * X’"},{"name":"transform_n_nonzero_coefs","type":"int, 0.1 * n_features by default","desc":"Number of nonzero coefficients to target in each column of the solution. This is only used by algorithm=’lars’ and algorithm=’omp’ and is overridden by alpha in the omp case."},{"name":"transform_alpha","type":"float, 1. by default","desc":"If algorithm=’lasso_lars’ or algorithm=’lasso_cd’, alpha is the penalty applied to the L1 norm. If algorithm=’threshold’, alpha is the absolute value of the threshold below which coefficients will be squashed to zero. If algorithm=’omp’, alpha is the tolerance parameter: the value of the reconstruction error targeted. In this case, it overrides n_nonzero_coefs."},{"name":"verbose","type":"bool, optional (default","desc":"To control the verbosity of the procedure."},{"name":"split_sign","type":"bool, False by default","desc":"Whether to split the sparse feature vector into the concatenation of its negative part and its positive part. This can improve the performance of downstream classifiers."},{"name":"random_state","type":"int, RandomState instance or None, optional (default=None)","desc":"If int, random_state is the seed used by the random number generator; If RandomState instance, random_state is the random number generator; If None, the random number generator is the RandomState instance used by np.random."}],"attrs":[{"name":"components_","type":"array, [n_components, n_features]","desc":"components extracted from the data"},{"name":"inner_stats_","type":"tuple of (A, B) ndarrays","desc":"Internal sufficient statistics that are kept by the algorithm. Keeping them is useful in online settings, to avoid loosing the history of the evolution, but they shouldn’t have any use for the end user. A (n_components, n_components) is the dictionary covariance matrix. B (n_features, n_components) is the data approximation matrix"},{"name":"n_iter_","type":"int","desc":"Number of iterations run."}]},{"method":"Mini Batch Sparse PCA","desc":"Mini-batch Sparse Principal Components Analysis<br />Finds the set of sparse components that can optimally reconstruct<br />the data.  The amount of sparseness is controllable by the coefficient<br />of the L1 penalty, given by the parameter alpha.<br />Read more in the User Guide.<br />","url":"http://scikit-learn.org/stable/modules/generated/sklearn.decomposition.MiniBatchSparsePCA.html#sklearn.decomposition.MiniBatchSparsePCA","params":[{"name":"n_components","type":"int,","desc":"number of sparse atoms to extract"},{"name":"alpha","type":"int,","desc":"Sparsity controlling parameter. Higher values lead to sparser components."},{"name":"ridge_alpha","type":"float,","desc":"Amount of ridge shrinkage to apply in order to improve conditioning when calling the transform method."},{"name":"n_iter","type":"int,","desc":"number of iterations to perform for each mini batch"},{"name":"callback","type":"callable or None, optional (default","desc":"callable that gets invoked every five iterations"},{"name":"batch_size","type":"int,","desc":"the number of features to take in each mini batch"},{"name":"verbose","type":"int","desc":"Controls the verbosity; the higher, the more messages. Defaults to 0."},{"name":"shuffle","type":"boolean,","desc":"whether to shuffle the data before splitting it in batches"},{"name":"n_jobs","type":"int,","desc":"number of parallel jobs to run, or -1 to autodetect."},{"name":"method","type":"{‘lars’, ‘cd’}","desc":"lars: uses the least angle regression method to solve the lasso problem (linear_model.lars_path) cd: uses the coordinate descent method to compute the Lasso solution (linear_model.Lasso). Lars will be faster if the estimated components are sparse."},{"name":"random_state","type":"int, RandomState instance or None, optional (default=None)","desc":"If int, random_state is the seed used by the random number generator; If RandomState instance, random_state is the random number generator; If None, the random number generator is the RandomState instance used by np.random."}],"attrs":[{"name":"components_","type":"array, [n_components, n_features]","desc":"Sparse components extracted from the data."},{"name":"error_","type":"array","desc":"Vector of errors at each iteration."},{"name":"n_iter_","type":"int","desc":"Number of iterations run."}]},{"method":"NMF","desc":"Non-Negative Matrix Factorization (NMF)<br />Find two non-negative matrices (W, H) whose product approximates the non-<br />negative matrix X. This factorization can be used for example for<br />dimensionality reduction, source separation or topic extraction.<br />The objective function is:<br />0.5 * ||X - WH||_Fro^2<br />+ alpha * l1_ratio * ||vec(W)||_1<br />+ alpha * l1_ratio * ||vec(H)||_1<br />+ 0.5 * alpha * (1 - l1_ratio) * ||W||_Fro^2<br />+ 0.5 * alpha * (1 - l1_ratio) * ||H||_Fro^2<br />Where:<br />||A||_Fro^2 = \\sum_{i,j} A_{ij}^2 (Frobenius norm)<br />||vec(A)||_1 = \\sum_{i,j} abs(A_{ij}) (Elementwise L1 norm)<br />For multiplicative-update (‘mu’) solver, the Frobenius norm<br />(0.5 * ||X - WH||_Fro^2) can be changed into another beta-divergence loss,<br />by changing the beta_loss parameter.<br />The objective function is minimized with an alternating minimization of W<br />and H.<br />Read more in the User Guide.<br />","url":"http://scikit-learn.org/stable/modules/generated/sklearn.decomposition.NMF.html#sklearn.decomposition.NMF","params":[{"name":"n_components","type":"int or None","desc":"Number of components, if n_components is not set all features are kept."},{"name":"init","type":"‘random’ | ‘nndsvd’ |  ‘nndsvda’ | ‘nndsvdar’ | ‘custom’","desc":"Method used to initialize the procedure. Default: ‘nndsvd’ if n_components < n_features, otherwise random. Valid options:   ‘random’: non-negative random matrices, scaled with: sqrt(X.mean() / n_components)    ‘nndsvd’: Nonnegative Double Singular Value Decomposition (NNDSVD) initialization (better for sparseness)    ‘nndsvda’: NNDSVD with zeros filled with the average of X (better when sparsity is not desired)    ‘nndsvdar’: NNDSVD with zeros filled with small random values (generally faster, less accurate alternative to NNDSVDa for when sparsity is not desired)   ‘custom’: use custom matrices W and H"},{"name":"solver","type":"‘cd’ | ‘mu’","desc":"Numerical solver to use: ‘cd’ is a Coordinate Descent solver. ‘mu’ is a Multiplicative Update solver.  New in version 0.17: Coordinate Descent solver.   New in version 0.19: Multiplicative Update solver."},{"name":"beta_loss","type":"float or string, default ‘frobenius’","desc":"String must be in {‘frobenius’, ‘kullback-leibler’, ‘itakura-saito’}. Beta divergence to be minimized, measuring the distance between X and the dot product WH. Note that values different from ‘frobenius’ (or 2) and ‘kullback-leibler’ (or 1) lead to significantly slower fits. Note that for beta_loss <= 0 (or ‘itakura-saito’), the input matrix X cannot contain zeros. Used only in ‘mu’ solver.  New in version 0.19."},{"name":"tol","type":"float, default","desc":"Tolerance of the stopping condition."},{"name":"max_iter","type":"integer, default","desc":"Maximum number of iterations before timing out."},{"name":"random_state","type":"int, RandomState instance or None, optional, default","desc":"If int, random_state is the seed used by the random number generator; If RandomState instance, random_state is the random number generator; If None, the random number generator is the RandomState instance used by np.random."},{"name":"alpha","type":"double, default","desc":"Constant that multiplies the regularization terms. Set it to zero to have no regularization.  New in version 0.17: alpha used in the Coordinate Descent solver."},{"name":"l1_ratio","type":"double, default","desc":"The regularization mixing parameter, with 0 <= l1_ratio <= 1. For l1_ratio = 0 the penalty is an elementwise L2 penalty (aka Frobenius Norm). For l1_ratio = 1 it is an elementwise L1 penalty. For 0 < l1_ratio < 1, the penalty is a combination of L1 and L2.  New in version 0.17: Regularization parameter l1_ratio used in the Coordinate Descent solver."},{"name":"verbose","type":"bool, default=False","desc":"Whether to be verbose."},{"name":"shuffle","type":"boolean, default","desc":"If true, randomize the order of coordinates in the CD solver.  New in version 0.17: shuffle parameter used in the Coordinate Descent solver."}],"attrs":[{"name":"components_","type":"array, [n_components, n_features]","desc":"Factorization matrix, sometimes called ‘dictionary’."},{"name":"reconstruction_err_","type":"number","desc":"Frobenius norm of the matrix difference, or beta-divergence, between the training data X and the reconstructed data WH from the fitted model."},{"name":"n_iter_","type":"int","desc":"Actual number of iterations."}]},{"method":"PCA","desc":"Principal component analysis (PCA)<br />Linear dimensionality reduction using Singular Value Decomposition of the<br />data to project it to a lower dimensional space.<br />It uses the LAPACK implementation of the full SVD or a randomized truncated<br />SVD by the method of Halko et al. 2009, depending on the shape of the input<br />data and the number of components to extract.<br />It can also use the scipy.sparse.linalg ARPACK implementation of the<br />truncated SVD.<br />Notice that this class does not support sparse input. See<br />TruncatedSVD for an alternative with sparse data.<br />Read more in the User Guide.<br />","url":"http://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html#sklearn.decomposition.PCA","params":[{"name":"n_components","type":"int, float, None or string","desc":"Number of components to keep. if n_components is not set all components are kept: n_components == min(n_samples, n_features)   if n_components == ‘mle’ and svd_solver == ‘full’, Minka’s MLE is used to guess the dimension if 0 < n_components < 1 and svd_solver == ‘full’, select the number of components such that the amount of variance that needs to be explained is greater than the percentage specified by n_components n_components cannot be equal to n_features for svd_solver == ‘arpack’."},{"name":"copy","type":"bool (default True)","desc":"If False, data passed to fit are overwritten and running fit(X).transform(X) will not yield the expected results, use fit_transform(X) instead."},{"name":"whiten","type":"bool, optional (default False)","desc":"When True (False by default) the components_ vectors are multiplied by the square root of n_samples and then divided by the singular values to ensure uncorrelated outputs with unit component-wise variances. Whitening will remove some information from the transformed signal (the relative variance scales of the components) but can sometime improve the predictive accuracy of the downstream estimators by making their data respect some hard-wired assumptions."},{"name":"svd_solver","type":"string {‘auto’, ‘full’, ‘arpack’, ‘randomized’}","desc":"auto : the solver is selected by a default policy based on X.shape and n_components: if the input data is larger than 500x500 and the number of components to extract is lower than 80% of the smallest dimension of the data, then the more efficient ‘randomized’ method is enabled. Otherwise the exact full SVD is computed and optionally truncated afterwards.  full : run exact full SVD calling the standard LAPACK solver via scipy.linalg.svd and select the components by postprocessing  arpack : run SVD truncated to n_components calling ARPACK solver via scipy.sparse.linalg.svds. It requires strictly 0 < n_components < X.shape[1]  randomized : run randomized SVD by the method of Halko et al.    New in version 0.18.0."},{"name":"tol","type":"float >= 0, optional (default .0)","desc":"Tolerance for singular values computed by svd_solver == ‘arpack’.  New in version 0.18.0."},{"name":"iterated_power","type":"int >= 0, or ‘auto’, (default ‘auto’)","desc":"Number of iterations for the power method computed by svd_solver == ‘randomized’.  New in version 0.18.0."},{"name":"random_state","type":"int, RandomState instance or None, optional (default None)","desc":"If int, random_state is the seed used by the random number generator; If RandomState instance, random_state is the random number generator; If None, the random number generator is the RandomState instance used by np.random. Used when svd_solver == ‘arpack’ or ‘randomized’.  New in version 0.18.0."}],"attrs":[{"name":"components_","type":"array, shape (n_components, n_features)","desc":"Principal axes in feature space, representing the directions of maximum variance in the data. The components are sorted by explained_variance_."},{"name":"explained_variance_","type":"array, shape (n_components,)","desc":"The amount of variance explained by each of the selected components. Equal to n_components largest eigenvalues of the covariance matrix of X.  New in version 0.18."},{"name":"explained_variance_ratio_","type":"array, shape (n_components,)","desc":"Percentage of variance explained by each of the selected components. If n_components is not set then all components are stored and the sum of explained variances is equal to 1.0."},{"name":"singular_values_","type":"array, shape (n_components,)","desc":"The singular values corresponding to each of the selected components. The singular values are equal to the 2-norms of the n_components variables in the lower-dimensional space."},{"name":"mean_","type":"array, shape (n_features,)","desc":"Per-feature empirical mean, estimated from the training set. Equal to X.mean(axis=0)."},{"name":"n_components_","type":"int","desc":"The estimated number of components. When n_components is set to ‘mle’ or a number between 0 and 1 (with svd_solver == ‘full’) this number is estimated from input data. Otherwise it equals the parameter n_components, or n_features if n_components is None."},{"name":"noise_variance_","type":"float","desc":"The estimated noise covariance following the Probabilistic PCA model from Tipping and Bishop 1999. See “Pattern Recognition and Machine Learning” by C. Bishop, 12.2.1 p. 574 or http://www.miketipping.com/papers/met-mppca.pdf. It is required to computed the estimated data covariance and score samples. Equal to the average of (min(n_features, n_samples) - n_components) smallest eigenvalues of the covariance matrix of X."}]},{"method":"Sparse PCA","desc":"Sparse Principal Components Analysis (SparsePCA)<br />Finds the set of sparse components that can optimally reconstruct<br />the data.  The amount of sparseness is controllable by the coefficient<br />of the L1 penalty, given by the parameter alpha.<br />Read more in the User Guide.<br />","url":"http://scikit-learn.org/stable/modules/generated/sklearn.decomposition.SparsePCA.html#sklearn.decomposition.SparsePCA","params":[{"name":"n_components","type":"int,","desc":"Number of sparse atoms to extract."},{"name":"alpha","type":"float,","desc":"Sparsity controlling parameter. Higher values lead to sparser components."},{"name":"ridge_alpha","type":"float,","desc":"Amount of ridge shrinkage to apply in order to improve conditioning when calling the transform method."},{"name":"max_iter","type":"int,","desc":"Maximum number of iterations to perform."},{"name":"tol","type":"float,","desc":"Tolerance for the stopping condition."},{"name":"method","type":"{‘lars’, ‘cd’}","desc":"lars: uses the least angle regression method to solve the lasso problem (linear_model.lars_path) cd: uses the coordinate descent method to compute the Lasso solution (linear_model.Lasso). Lars will be faster if the estimated components are sparse."},{"name":"n_jobs","type":"int,","desc":"Number of parallel jobs to run."},{"name":"U_init","type":"array of shape (n_samples, n_components),","desc":"Initial values for the loadings for warm restart scenarios."},{"name":"V_init","type":"array of shape (n_components, n_features),","desc":"Initial values for the components for warm restart scenarios."},{"name":"verbose","type":"int","desc":"Controls the verbosity; the higher, the more messages. Defaults to 0."},{"name":"random_state","type":"int, RandomState instance or None, optional (default=None)","desc":"If int, random_state is the seed used by the random number generator; If RandomState instance, random_state is the random number generator; If None, the random number generator is the RandomState instance used by np.random."}],"attrs":[{"name":"components_","type":"array, [n_components, n_features]","desc":"Sparse components extracted from the data."},{"name":"error_","type":"array","desc":"Vector of errors at each iteration."},{"name":"n_iter_","type":"int","desc":"Number of iterations run."}]},{"method":"Sparse Coder","desc":"Sparse coding<br />Finds a sparse representation of data against a fixed, precomputed<br />dictionary.<br />Each row of the result is the solution to a sparse coding problem.<br />The goal is to find a sparse array code such that:<br />X ~= code * dictionary<br />Read more in the User Guide.<br />","url":"http://scikit-learn.org/stable/modules/generated/sklearn.decomposition.SparseCoder.html#sklearn.decomposition.SparseCoder","params":[{"name":"dictionary","type":"array, [n_components, n_features]","desc":"The dictionary atoms used for sparse coding. Lines are assumed to be normalized to unit norm."},{"name":"transform_algorithm","type":"{‘lasso_lars’, ‘lasso_cd’, ‘lars’, ‘omp’,     ‘threshold’}","desc":"Algorithm used to transform the data: lars: uses the least angle regression method (linear_model.lars_path) lasso_lars: uses Lars to compute the Lasso solution lasso_cd: uses the coordinate descent method to compute the Lasso solution (linear_model.Lasso). lasso_lars will be faster if the estimated components are sparse. omp: uses orthogonal matching pursuit to estimate the sparse solution threshold: squashes to zero all coefficients less than alpha from the projection dictionary * X'"},{"name":"transform_n_nonzero_coefs","type":"int, 0.1 * n_features by default","desc":"Number of nonzero coefficients to target in each column of the solution. This is only used by algorithm=’lars’ and algorithm=’omp’ and is overridden by alpha in the omp case."},{"name":"transform_alpha","type":"float, 1. by default","desc":"If algorithm=’lasso_lars’ or algorithm=’lasso_cd’, alpha is the penalty applied to the L1 norm. If algorithm=’threshold’, alpha is the absolute value of the threshold below which coefficients will be squashed to zero. If algorithm=’omp’, alpha is the tolerance parameter: the value of the reconstruction error targeted. In this case, it overrides n_nonzero_coefs."},{"name":"split_sign","type":"bool, False by default","desc":"Whether to split the sparse feature vector into the concatenation of its negative part and its positive part. This can improve the performance of downstream classifiers."},{"name":"n_jobs","type":"int,","desc":"number of parallel jobs to run"}],"attrs":[{"name":"components_","type":"array, [n_components, n_features]","desc":"The unchanged dictionary atoms"}]},{"method":"Truncated SVD","desc":"Dimensionality reduction using truncated SVD (aka LSA).<br />This transformer performs linear dimensionality reduction by means of<br />truncated singular value decomposition (SVD). Contrary to PCA, this<br />estimator does not center the data before computing the singular value<br />decomposition. This means it can work with scipy.sparse matrices<br />efficiently.<br />In particular, truncated SVD works on term count/tf-idf matrices as<br />returned by the vectorizers in sklearn.feature_extraction.text. In that<br />context, it is known as latent semantic analysis (LSA).<br />This estimator supports two algorithms: a fast randomized SVD solver, and<br />a “naive” algorithm that uses ARPACK as an eigensolver on (X * X.T) or<br />(X.T * X), whichever is more efficient.<br />Read more in the User Guide.<br />","url":"http://scikit-learn.org/stable/modules/generated/sklearn.decomposition.TruncatedSVD.html#sklearn.decomposition.TruncatedSVD","params":[{"name":"n_components","type":"int, default = 2","desc":"Desired dimensionality of output data. Must be strictly less than the number of features. The default value is useful for visualisation. For LSA, a value of 100 is recommended."},{"name":"algorithm","type":"string, default = “randomized”","desc":"SVD solver to use. Either “arpack” for the ARPACK wrapper in SciPy (scipy.sparse.linalg.svds), or “randomized” for the randomized algorithm due to Halko (2009)."},{"name":"n_iter","type":"int, optional (default 5)","desc":"Number of iterations for randomized SVD solver. Not used by ARPACK. The default is larger than the default in randomized_svd to handle sparse matrices that may have large slowly decaying spectrum."},{"name":"random_state","type":"int, RandomState instance or None, optional, default = None","desc":"If int, random_state is the seed used by the random number generator; If RandomState instance, random_state is the random number generator; If None, the random number generator is the RandomState instance used by np.random."},{"name":"tol","type":"float, optional","desc":"Tolerance for ARPACK. 0 means machine precision. Ignored by randomized SVD solver."}],"attrs":[{"name":"components_","type":"array, shape (n_components, n_features)","desc":"explained_variance_ : array, shape (n_components,)"},{"name":"The variance of the training samples transformed by a projection to\neach component.","type":"","desc":"explained_variance_ratio_ : array, shape (n_components,)"},{"name":"Percentage of variance explained by each of the selected components.","type":"","desc":"singular_values_ : array, shape (n_components,)"},{"name":"The singular values corresponding to each of the selected components.\nThe singular values are equal to the 2-norms of the n_components\nvariables in the lower-dimensional space.","type":"","desc":""}]},{"method":"Dict Learning","desc":"Solves a dictionary learning matrix factorization problem.<br />Finds the best dictionary and the corresponding sparse code for<br />approximating the data matrix X by solving:<br />(U^*, V^*) = argmin 0.5 || X - U V ||_2^2 + alpha * || U ||_1<br />(U,V)<br />with || V_k ||_2 = 1 for all  0 <= k < n_components<br />where V is the dictionary and U is the sparse code.<br />Read more in the User Guide.<br />","url":"http://scikit-learn.org/stable/modules/generated/sklearn.decomposition.dict_learning.html#sklearn.decomposition.dict_learning","params":[{"name":"X","type":"array of shape (n_samples, n_features)","desc":"Data matrix."},{"name":"n_components","type":"int,","desc":"Number of dictionary atoms to extract."},{"name":"alpha","type":"int,","desc":"Sparsity controlling parameter."},{"name":"max_iter","type":"int,","desc":"Maximum number of iterations to perform."},{"name":"tol","type":"float,","desc":"Tolerance for the stopping condition."},{"name":"method","type":"{‘lars’, ‘cd’}","desc":"lars: uses the least angle regression method to solve the lasso problem (linear_model.lars_path) cd: uses the coordinate descent method to compute the Lasso solution (linear_model.Lasso). Lars will be faster if the estimated components are sparse."},{"name":"n_jobs","type":"int,","desc":"Number of parallel jobs to run, or -1 to autodetect."},{"name":"dict_init","type":"array of shape (n_components, n_features),","desc":"Initial value for the dictionary for warm restart scenarios."},{"name":"code_init","type":"array of shape (n_samples, n_components),","desc":"Initial value for the sparse code for warm restart scenarios."},{"name":"callback","type":"callable or None, optional (default","desc":"Callable that gets invoked every five iterations"},{"name":"verbose","type":"bool, optional (default","desc":"To control the verbosity of the procedure."},{"name":"random_state","type":"int, RandomState instance or None, optional (default=None)","desc":"If int, random_state is the seed used by the random number generator; If RandomState instance, random_state is the random number generator; If None, the random number generator is the RandomState instance used by np.random."},{"name":"return_n_iter","type":"bool","desc":"Whether or not to return the number of iterations."}],"attrs":[{"name":"code","type":"array of shape (n_samples, n_components)","desc":"The sparse code factor in the matrix factorization."},{"name":"dictionary","type":"array of shape (n_components, n_features),","desc":"The dictionary factor in the matrix factorization."},{"name":"errors","type":"array","desc":"Vector of errors at each iteration."},{"name":"n_iter","type":"int","desc":"Number of iterations run. Returned only if return_n_iter is set to True."}]},{"method":"Dict Learning Online","desc":"Solves a dictionary learning matrix factorization problem online.<br />Finds the best dictionary and the corresponding sparse code for<br />approximating the data matrix X by solving:<br />(U^*, V^*) = argmin 0.5 || X - U V ||_2^2 + alpha * || U ||_1<br />(U,V)<br />with || V_k ||_2 = 1 for all  0 <= k < n_components<br />where V is the dictionary and U is the sparse code. This is<br />accomplished by repeatedly iterating over mini-batches by slicing<br />the input data.<br />Read more in the User Guide.<br />","url":"http://scikit-learn.org/stable/modules/generated/sklearn.decomposition.dict_learning_online.html","params":[{"name":"X","type":"array of shape (n_samples, n_features)","desc":"Data matrix."},{"name":"n_components","type":"int,","desc":"Number of dictionary atoms to extract."},{"name":"alpha","type":"float,","desc":"Sparsity controlling parameter."},{"name":"n_iter","type":"int,","desc":"Number of iterations to perform."},{"name":"return_code","type":"boolean,","desc":"Whether to also return the code U or just the dictionary V."},{"name":"dict_init","type":"array of shape (n_components, n_features),","desc":"Initial value for the dictionary for warm restart scenarios."},{"name":"callback","type":"callable or None, optional (default","desc":"callable that gets invoked every five iterations"},{"name":"batch_size","type":"int,","desc":"The number of samples to take in each batch."},{"name":"verbose","type":"bool, optional (default","desc":"To control the verbosity of the procedure."},{"name":"shuffle","type":"boolean,","desc":"Whether to shuffle the data before splitting it in batches."},{"name":"n_jobs","type":"int,","desc":"Number of parallel jobs to run, or -1 to autodetect."},{"name":"method","type":"{‘lars’, ‘cd’}","desc":"lars: uses the least angle regression method to solve the lasso problem (linear_model.lars_path) cd: uses the coordinate descent method to compute the Lasso solution (linear_model.Lasso). Lars will be faster if the estimated components are sparse."},{"name":"iter_offset","type":"int, default 0","desc":"Number of previous iterations completed on the dictionary used for initialization."},{"name":"random_state","type":"int, RandomState instance or None, optional (default=None)","desc":"If int, random_state is the seed used by the random number generator; If RandomState instance, random_state is the random number generator; If None, the random number generator is the RandomState instance used by np.random."},{"name":"return_inner_stats","type":"boolean, optional","desc":"Return the inner statistics A (dictionary covariance) and B (data approximation). Useful to restart the algorithm in an online setting. If return_inner_stats is True, return_code is ignored"},{"name":"inner_stats","type":"tuple of (A, B) ndarrays","desc":"Inner sufficient statistics that are kept by the algorithm. Passing them at initialization is useful in online settings, to avoid loosing the history of the evolution. A (n_components, n_components) is the dictionary covariance matrix. B (n_features, n_components) is the data approximation matrix"},{"name":"return_n_iter","type":"bool","desc":"Whether or not to return the number of iterations."}],"attrs":[{"name":"code","type":"array of shape (n_samples, n_components),","desc":"the sparse code (only returned if return_code=True)"},{"name":"dictionary","type":"array of shape (n_components, n_features),","desc":"the solutions to the dictionary learning problem"},{"name":"n_iter","type":"int","desc":"Number of iterations run. Returned only if return_n_iter is set to True."}]},{"method":"Fastica","desc":"Perform Fast Independent Component Analysis.<br />Read more in the User Guide.<br />","url":"http://scikit-learn.org/stable/modules/generated/sklearn.decomposition.fastica.html","params":[{"name":"X","type":"array-like, shape (n_samples, n_features)","desc":"Training vector, where n_samples is the number of samples and n_features is the number of features."},{"name":"n_components","type":"int, optional","desc":"Number of components to extract. If None no dimension reduction is performed."},{"name":"algorithm","type":"{‘parallel’, ‘deflation’}, optional","desc":"Apply a parallel or deflational FASTICA algorithm."},{"name":"whiten","type":"boolean, optional","desc":"If True perform an initial whitening of the data. If False, the data is assumed to have already been preprocessed: it should be centered, normed and white. Otherwise you will get incorrect results. In this case the parameter n_components will be ignored."},{"name":"fun","type":"string or function, optional. Default","desc":"The functional form of the G function used in the approximation to neg-entropy. Could be either ‘logcosh’, ‘exp’, or ‘cube’. You can also provide your own function. It should return a tuple containing the value of the function, and of its derivative, in the point. Example:  def my_g(x): return x ** 3, 3 * x ** 2"},{"name":"fun_args","type":"dictionary, optional","desc":"Arguments to send to the functional form. If empty or None and if fun=’logcosh’, fun_args will take value {‘alpha’ : 1.0}"},{"name":"max_iter","type":"int, optional","desc":"Maximum number of iterations to perform."},{"name":"tol","type":"float, optional","desc":"A positive scalar giving the tolerance at which the un-mixing matrix is considered to have converged."},{"name":"w_init","type":"(n_components, n_components) array, optional","desc":"Initial un-mixing array of dimension (n.comp,n.comp). If None (default) then an array of normal r.v.’s is used."},{"name":"random_state","type":"int, RandomState instance or None, optional (default=None)","desc":"If int, random_state is the seed used by the random number generator; If RandomState instance, random_state is the random number generator; If None, the random number generator is the RandomState instance used by np.random."},{"name":"return_X_mean","type":"bool, optional","desc":"If True, X_mean is returned too."},{"name":"compute_sources","type":"bool, optional","desc":"If False, sources are not computed, but only the rotation matrix. This can save memory when working with big data. Defaults to True."},{"name":"return_n_iter","type":"bool, optional","desc":"Whether or not to return the number of iterations."}],"attrs":[{"name":"K","type":"array, shape (n_components, n_features) | None.","desc":"If whiten is ‘True’, K is the pre-whitening matrix that projects data onto the first n_components principal components. If whiten is ‘False’, K is ‘None’."},{"name":"W","type":"array, shape (n_components, n_components)","desc":"Estimated un-mixing matrix. The mixing matrix can be obtained by: w = np.dot(W, K.T) A = w.T * (w * w.T).I"},{"name":"S","type":"array, shape (n_samples, n_components) | None","desc":"Estimated source matrix"},{"name":"X_mean","type":"array, shape (n_features, )","desc":"The mean over features. Returned only if return_X_mean is True."},{"name":"n_iter","type":"int","desc":"If the algorithm is “deflation”, n_iter is the maximum number of iterations run across all components. Else they are just the number of iterations taken to converge. This is returned only when return_n_iter is set to True."}]},{"method":"Sparse Encode","desc":"Sparse coding<br />Each row of the result is the solution to a sparse coding problem.<br />The goal is to find a sparse array code such that:<br />X ~= code * dictionary<br />Read more in the User Guide.<br />","url":"http://scikit-learn.org/stable/modules/generated/sklearn.decomposition.sparse_encode.html","params":[{"name":"X","type":"array of shape (n_samples, n_features)","desc":"Data matrix"},{"name":"dictionary","type":"array of shape (n_components, n_features)","desc":"The dictionary matrix against which to solve the sparse coding of the data. Some of the algorithms assume normalized rows for meaningful output."},{"name":"gram","type":"array, shape=(n_components, n_components)","desc":"Precomputed Gram matrix, dictionary * dictionary’"},{"name":"cov","type":"array, shape=(n_components, n_samples)","desc":"Precomputed covariance, dictionary’ * X"},{"name":"algorithm","type":"{‘lasso_lars’, ‘lasso_cd’, ‘lars’, ‘omp’, ‘threshold’}","desc":"lars: uses the least angle regression method (linear_model.lars_path) lasso_lars: uses Lars to compute the Lasso solution lasso_cd: uses the coordinate descent method to compute the Lasso solution (linear_model.Lasso). lasso_lars will be faster if the estimated components are sparse. omp: uses orthogonal matching pursuit to estimate the sparse solution threshold: squashes to zero all coefficients less than alpha from the projection dictionary * X’"},{"name":"n_nonzero_coefs","type":"int, 0.1 * n_features by default","desc":"Number of nonzero coefficients to target in each column of the solution. This is only used by algorithm=’lars’ and algorithm=’omp’ and is overridden by alpha in the omp case."},{"name":"alpha","type":"float, 1. by default","desc":"If algorithm=’lasso_lars’ or algorithm=’lasso_cd’, alpha is the penalty applied to the L1 norm. If algorithm=’threshold’, alpha is the absolute value of the threshold below which coefficients will be squashed to zero. If algorithm=’omp’, alpha is the tolerance parameter: the value of the reconstruction error targeted. In this case, it overrides n_nonzero_coefs."},{"name":"copy_cov","type":"boolean, optional","desc":"Whether to copy the precomputed covariance matrix; if False, it may be overwritten."},{"name":"init","type":"array of shape (n_samples, n_components)","desc":"Initialization value of the sparse codes. Only used if algorithm=’lasso_cd’."},{"name":"max_iter","type":"int, 1000 by default","desc":"Maximum number of iterations to perform if algorithm=’lasso_cd’."},{"name":"n_jobs","type":"int, optional","desc":"Number of parallel jobs to run."},{"name":"check_input","type":"boolean, optional","desc":"If False, the input arrays X and dictionary will not be checked."},{"name":"verbose","type":"int, optional","desc":"Controls the verbosity; the higher, the more messages. Defaults to 0."}],"attrs":[{"name":"code","type":"array of shape (n_samples, n_components)","desc":"The sparse codes"}]},{"method":"Affinity Propagation","desc":"Perform Affinity Propagation Clustering of data.<br />Read more in the User Guide.<br />","url":"http://scikit-learn.org/stable/modules/generated/sklearn.cluster.AffinityPropagation.html#sklearn.cluster.AffinityPropagation","params":[{"name":"damping","type":"float, optional, default","desc":"Damping factor (between 0.5 and 1) is the extent to which the current value is maintained relative to incoming values (weighted 1 - damping). This in order to avoid numerical oscillations when updating these values (messages)."},{"name":"max_iter","type":"int, optional, default","desc":"Maximum number of iterations."},{"name":"convergence_iter","type":"int, optional, default","desc":"Number of iterations with no change in the number of estimated clusters that stops the convergence."},{"name":"copy","type":"boolean, optional, default","desc":"Make a copy of input data."},{"name":"preference","type":"array-like, shape (n_samples,) or float, optional","desc":"Preferences for each point - points with larger values of preferences are more likely to be chosen as exemplars. The number of exemplars, ie of clusters, is influenced by the input preferences value. If the preferences are not passed as arguments, they will be set to the median of the input similarities."},{"name":"affinity","type":"string, optional, default=``euclidean``","desc":"Which affinity to use. At the moment precomputed and euclidean are supported. euclidean uses the negative squared euclidean distance between points."},{"name":"verbose","type":"boolean, optional, default","desc":"Whether to be verbose."}],"attrs":[{"name":"cluster_centers_indices_","type":"array, shape (n_clusters,)","desc":"Indices of cluster centers"},{"name":"cluster_centers_","type":"array, shape (n_clusters, n_features)","desc":"Cluster centers (if affinity != precomputed)."},{"name":"labels_","type":"array, shape (n_samples,)","desc":"Labels of each point"},{"name":"affinity_matrix_","type":"array, shape (n_samples, n_samples)","desc":"Stores the affinity matrix used in fit."},{"name":"n_iter_","type":"int","desc":"Number of iterations taken to converge."}]},{"method":"Agglomerative Clustering","desc":"Agglomerative Clustering<br />Recursively merges the pair of clusters that minimally increases<br />a given linkage distance.<br />Read more in the User Guide.<br />","url":"http://scikit-learn.org/stable/modules/generated/sklearn.cluster.AgglomerativeClustering.html#sklearn.cluster.AgglomerativeClustering","params":[{"name":"n_clusters","type":"int, default=2","desc":"The number of clusters to find."},{"name":"affinity","type":"string or callable, default","desc":"Metric used to compute the linkage. Can be “euclidean”, “l1”, “l2”, “manhattan”, “cosine”, or ‘precomputed’. If linkage is “ward”, only “euclidean” is accepted."},{"name":"memory","type":"None, str or object with the joblib.Memory interface, optional","desc":"Used to cache the output of the computation of the tree. By default, no caching is done. If a string is given, it is the path to the caching directory."},{"name":"connectivity","type":"array-like or callable, optional","desc":"Connectivity matrix. Defines for each sample the neighboring samples following a given structure of the data. This can be a connectivity matrix itself or a callable that transforms the data into a connectivity matrix, such as derived from kneighbors_graph. Default is None, i.e, the hierarchical clustering algorithm is unstructured."},{"name":"compute_full_tree","type":"bool or ‘auto’ (optional)","desc":"Stop early the construction of the tree at n_clusters. This is useful to decrease computation time if the number of clusters is not small compared to the number of samples. This option is useful only when specifying a connectivity matrix. Note also that when varying the number of clusters and using caching, it may be advantageous to compute the full tree."},{"name":"linkage","type":"{“ward”, “complete”, “average”}, optional, default","desc":"Which linkage criterion to use. The linkage criterion determines which distance to use between sets of observation. The algorithm will merge the pairs of cluster that minimize this criterion.  ward minimizes the variance of the clusters being merged. average uses the average of the distances of each observation of the two sets. complete or maximum linkage uses the maximum distances between all observations of the two sets."},{"name":"pooling_func","type":"callable, default=np.mean","desc":"This combines the values of agglomerated features into a single value, and should accept an array of shape [M, N] and the keyword argument axis=1, and reduce it to an array of size [M]."}],"attrs":[{"name":"labels_","type":"array [n_samples]","desc":"cluster labels for each point"},{"name":"n_leaves_","type":"int","desc":"Number of leaves in the hierarchical tree."},{"name":"n_components_","type":"int","desc":"The estimated number of connected components in the graph."},{"name":"children_","type":"array-like, shape (n_nodes-1, 2)","desc":"The children of each non-leaf node. Values less than n_samples correspond to leaves of the tree which are the original samples. A node i greater than or equal to n_samples is a non-leaf node and has children children_[i - n_samples]. Alternatively at the i-th iteration, children[i][0] and children[i][1] are merged to form node n_samples + i"}]},{"method":"Birch","desc":"Implements the Birch clustering algorithm.<br />It is a memory-efficient, online-learning algorithm provided as an<br />alternative to MiniBatchKMeans. It constructs a tree<br />data structure with the cluster centroids being read off the leaf.<br />These can be either the final cluster centroids or can be provided as input<br />to another clustering algorithm such as AgglomerativeClustering.<br />Read more in the User Guide.<br />","url":"http://scikit-learn.org/stable/modules/generated/sklearn.cluster.Birch.html#sklearn.cluster.Birch","params":[{"name":"threshold","type":"float, default 0.5","desc":"The radius of the subcluster obtained by merging a new sample and the closest subcluster should be lesser than the threshold. Otherwise a new subcluster is started. Setting this value to be very low promotes splitting and vice-versa."},{"name":"branching_factor","type":"int, default 50","desc":"Maximum number of CF subclusters in each node. If a new samples enters such that the number of subclusters exceed the branching_factor then that node is split into two nodes with the subclusters redistributed in each. The parent subcluster of that node is removed and two new subclusters are added as parents of the 2 split nodes."},{"name":"n_clusters","type":"int, instance of sklearn.cluster model, default 3","desc":"Number of clusters after the final clustering step, which treats the subclusters from the leaves as new samples.  None : the final clustering step is not performed and the subclusters are returned as they are. sklearn.cluster Estimator : If a model is provided, the model is fit treating the subclusters as new samples and the initial data is mapped to the label of the closest subcluster. int : the model fit is AgglomerativeClustering with n_clusters set to be equal to the int."},{"name":"compute_labels","type":"bool, default True","desc":"Whether or not to compute labels for each fit."},{"name":"copy","type":"bool, default True","desc":"Whether or not to make a copy of the given data. If set to False, the initial data will be overwritten."}],"attrs":[{"name":"root_","type":"_CFNode","desc":"Root of the CFTree."},{"name":"dummy_leaf_","type":"_CFNode","desc":"Start pointer to all the leaves."},{"name":"subcluster_centers_","type":"ndarray,","desc":"Centroids of all subclusters read directly from the leaves."},{"name":"subcluster_labels_","type":"ndarray,","desc":"Labels assigned to the centroids of the subclusters after they are clustered globally."},{"name":"labels_","type":"ndarray, shape (n_samples,)","desc":"Array of labels assigned to the input data. if partial_fit is used instead of fit, they are assigned to the last batch of data."}]},{"method":"DBSCAN","desc":"Perform DBSCAN clustering from vector array or distance matrix.<br />DBSCAN - Density-Based Spatial Clustering of Applications with Noise.<br />Finds core samples of high density and expands clusters from them.<br />Good for data which contains clusters of similar density.<br />Read more in the User Guide.<br />","url":"http://scikit-learn.org/stable/modules/generated/sklearn.cluster.DBSCAN.html#sklearn.cluster.DBSCAN","params":[{"name":"eps","type":"float, optional","desc":"The maximum distance between two samples for them to be considered as in the same neighborhood."},{"name":"min_samples","type":"int, optional","desc":"The number of samples (or total weight) in a neighborhood for a point to be considered as a core point. This includes the point itself."},{"name":"metric","type":"string, or callable","desc":"The metric to use when calculating distance between instances in a feature array. If metric is a string or callable, it must be one of the options allowed by metrics.pairwise.calculate_distance for its metric parameter. If metric is “precomputed”, X is assumed to be a distance matrix and must be square. X may be a sparse matrix, in which case only “nonzero” elements may be considered neighbors for DBSCAN.  New in version 0.17: metric precomputed to accept precomputed sparse matrix."},{"name":"metric_params","type":"dict, optional","desc":"Additional keyword arguments for the metric function.  New in version 0.19."},{"name":"algorithm","type":"{‘auto’, ‘ball_tree’, ‘kd_tree’, ‘brute’}, optional","desc":"The algorithm to be used by the NearestNeighbors module to compute pointwise distances and find nearest neighbors. See NearestNeighbors module documentation for details."},{"name":"leaf_size","type":"int, optional (default = 30)","desc":"Leaf size passed to BallTree or cKDTree. This can affect the speed of the construction and query, as well as the memory required to store the tree. The optimal value depends on the nature of the problem."},{"name":"p","type":"float, optional","desc":"The power of the Minkowski metric to be used to calculate distance between points."},{"name":"n_jobs","type":"int, optional (default = 1)","desc":"The number of parallel jobs to run. If -1, then the number of jobs is set to the number of CPU cores."}],"attrs":[{"name":"core_sample_indices_","type":"array, shape = [n_core_samples]","desc":"Indices of core samples."},{"name":"components_","type":"array, shape = [n_core_samples, n_features]","desc":"Copy of each core sample found by training."},{"name":"labels_","type":"array, shape = [n_samples]","desc":"Cluster labels for each point in the dataset given to fit(). Noisy samples are given the label -1."}]},{"method":"Feature Agglomeration","desc":"Agglomerate features.<br />Similar to AgglomerativeClustering, but recursively merges features<br />instead of samples.<br />Read more in the User Guide.<br />","url":"http://scikit-learn.org/stable/modules/generated/sklearn.cluster.FeatureAgglomeration.html#sklearn.cluster.FeatureAgglomeration","params":[{"name":"n_clusters","type":"int, default 2","desc":"The number of clusters to find."},{"name":"affinity","type":"string or callable, default “euclidean”","desc":"Metric used to compute the linkage. Can be “euclidean”, “l1”, “l2”, “manhattan”, “cosine”, or ‘precomputed’. If linkage is “ward”, only “euclidean” is accepted."},{"name":"memory","type":"None, str or object with the joblib.Memory interface, optional","desc":"Used to cache the output of the computation of the tree. By default, no caching is done. If a string is given, it is the path to the caching directory."},{"name":"connectivity","type":"array-like or callable, optional","desc":"Connectivity matrix. Defines for each feature the neighboring features following a given structure of the data. This can be a connectivity matrix itself or a callable that transforms the data into a connectivity matrix, such as derived from kneighbors_graph. Default is None, i.e, the hierarchical clustering algorithm is unstructured."},{"name":"compute_full_tree","type":"bool or ‘auto’, optional, default “auto”","desc":"Stop early the construction of the tree at n_clusters. This is useful to decrease computation time if the number of clusters is not small compared to the number of features. This option is useful only when specifying a connectivity matrix. Note also that when varying the number of clusters and using caching, it may be advantageous to compute the full tree."},{"name":"linkage","type":"{“ward”, “complete”, “average”}, optional, default “ward”","desc":"Which linkage criterion to use. The linkage criterion determines which distance to use between sets of features. The algorithm will merge the pairs of cluster that minimize this criterion.  ward minimizes the variance of the clusters being merged. average uses the average of the distances of each feature of the two sets. complete or maximum linkage uses the maximum distances between all features of the two sets."},{"name":"pooling_func","type":"callable, default np.mean","desc":"This combines the values of agglomerated features into a single value, and should accept an array of shape [M, N] and the keyword argument axis=1, and reduce it to an array of size [M]."}],"attrs":[{"name":"labels_","type":"array-like, (n_features,)","desc":"cluster labels for each feature."},{"name":"n_leaves_","type":"int","desc":"Number of leaves in the hierarchical tree."},{"name":"n_components_","type":"int","desc":"The estimated number of connected components in the graph."},{"name":"children_","type":"array-like, shape (n_nodes-1, 2)","desc":"The children of each non-leaf node. Values less than n_features correspond to leaves of the tree which are the original samples. A node i greater than or equal to n_features is a non-leaf node and has children children_[i - n_features]. Alternatively at the i-th iteration, children[i][0] and children[i][1] are merged to form node n_features + i"}]},{"method":"K Means","desc":"K-Means clustering<br />Read more in the User Guide.<br />","url":"http://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html#sklearn.cluster.KMeans","params":[{"name":"n_clusters","type":"int, optional, default","desc":"The number of clusters to form as well as the number of centroids to generate."},{"name":"init","type":"{‘k-means++’, ‘random’ or an ndarray}","desc":"Method for initialization, defaults to ‘k-means++’: ‘k-means++’ : selects initial cluster centers for k-mean clustering in a smart way to speed up convergence. See section Notes in k_init for more details. ‘random’: choose k observations (rows) at random from data for the initial centroids. If an ndarray is passed, it should be of shape (n_clusters, n_features) and gives the initial centers."},{"name":"n_init","type":"int, default","desc":"Number of time the k-means algorithm will be run with different centroid seeds. The final results will be the best output of n_init consecutive runs in terms of inertia."},{"name":"max_iter","type":"int, default","desc":"Maximum number of iterations of the k-means algorithm for a single run."},{"name":"tol","type":"float, default","desc":"Relative tolerance with regards to inertia to declare convergence"},{"name":"precompute_distances","type":"{‘auto’, True, False}","desc":"Precompute distances (faster but takes more memory). ‘auto’ : do not precompute distances if n_samples * n_clusters > 12 million. This corresponds to about 100MB overhead per job using double precision. True : always precompute distances False : never precompute distances"},{"name":"verbose","type":"int, default 0","desc":"Verbosity mode."},{"name":"random_state","type":"int, RandomState instance or None, optional, default","desc":"If int, random_state is the seed used by the random number generator; If RandomState instance, random_state is the random number generator; If None, the random number generator is the RandomState instance used by np.random."},{"name":"copy_x","type":"boolean, default True","desc":"When pre-computing distances it is more numerically accurate to center the data first.  If copy_x is True, then the original data is not modified.  If False, the original data is modified, and put back before the function returns, but small numerical differences may be introduced by subtracting and then adding the data mean."},{"name":"n_jobs","type":"int","desc":"The number of jobs to use for the computation. This works by computing each of the n_init runs in parallel. If -1 all CPUs are used. If 1 is given, no parallel computing code is used at all, which is useful for debugging. For n_jobs below -1, (n_cpus + 1 + n_jobs) are used. Thus for n_jobs = -2, all CPUs but one are used."},{"name":"algorithm","type":"“auto”, “full” or “elkan”, default=”auto”","desc":"K-means algorithm to use. The classical EM-style algorithm is “full”. The “elkan” variation is more efficient by using the triangle inequality, but currently doesn’t support sparse data. “auto” chooses “elkan” for dense data and “full” for sparse data."}],"attrs":[{"name":"cluster_centers_","type":"array, [n_clusters, n_features]","desc":"Coordinates of cluster centers"},{"name":"labels_","type":"","desc":"Labels of each point"},{"name":"inertia_","type":"float","desc":"Sum of squared distances of samples to their closest cluster center."}]},{"method":"Mini Batch KMeans","desc":"Mini-Batch K-Means clustering<br />Read more in the User Guide.<br />","url":"http://scikit-learn.org/stable/modules/generated/sklearn.cluster.MiniBatchKMeans.html#sklearn.cluster.MiniBatchKMeans","params":[{"name":"n_clusters","type":"int, optional, default","desc":"The number of clusters to form as well as the number of centroids to generate."},{"name":"init","type":"{‘k-means++’, ‘random’ or an ndarray}, default","desc":"Method for initialization, defaults to ‘k-means++’: ‘k-means++’ : selects initial cluster centers for k-mean clustering in a smart way to speed up convergence. See section Notes in k_init for more details. ‘random’: choose k observations (rows) at random from data for the initial centroids. If an ndarray is passed, it should be of shape (n_clusters, n_features) and gives the initial centers."},{"name":"max_iter","type":"int, optional","desc":"Maximum number of iterations over the complete dataset before stopping independently of any early stopping criterion heuristics."},{"name":"batch_size","type":"int, optional, default","desc":"Size of the mini batches."},{"name":"verbose","type":"boolean, optional","desc":"Verbosity mode."},{"name":"compute_labels","type":"boolean, default=True","desc":"Compute label assignment and inertia for the complete dataset once the minibatch optimization has converged in fit."},{"name":"random_state","type":"int, RandomState instance or None, optional, default","desc":"If int, random_state is the seed used by the random number generator; If RandomState instance, random_state is the random number generator; If None, the random number generator is the RandomState instance used by np.random."},{"name":"tol","type":"float, default","desc":"Control early stopping based on the relative center changes as measured by a smoothed, variance-normalized of the mean center squared position changes. This early stopping heuristics is closer to the one used for the batch variant of the algorithms but induces a slight computational and memory overhead over the inertia heuristic. To disable convergence detection based on normalized center change, set tol to 0.0 (default)."},{"name":"max_no_improvement","type":"int, default","desc":"Control early stopping based on the consecutive number of mini batches that does not yield an improvement on the smoothed inertia. To disable convergence detection based on inertia, set max_no_improvement to None."},{"name":"init_size","type":"int, optional, default","desc":"Number of samples to randomly sample for speeding up the initialization (sometimes at the expense of accuracy): the only algorithm is initialized by running a batch KMeans on a random subset of the data. This needs to be larger than n_clusters."},{"name":"n_init","type":"int, default=3","desc":"Number of random initializations that are tried. In contrast to KMeans, the algorithm is only run once, using the best of the n_init initializations as measured by inertia."},{"name":"reassignment_ratio","type":"float, default","desc":"Control the fraction of the maximum number of counts for a center to be reassigned. A higher value means that low count centers are more easily reassigned, which means that the model will take longer to converge, but should converge in a better clustering."}],"attrs":[{"name":"cluster_centers_","type":"array, [n_clusters, n_features]","desc":"Coordinates of cluster centers"},{"name":"labels_","type":"","desc":"Labels of each point (if compute_labels is set to True)."},{"name":"inertia_","type":"float","desc":"The value of the inertia criterion associated with the chosen partition (if compute_labels is set to True). The inertia is defined as the sum of square distances of samples to their nearest neighbor."}]},{"method":"Mean Shift","desc":"Mean shift clustering using a flat kernel.<br />Mean shift clustering aims to discover “blobs” in a smooth density of<br />samples. It is a centroid-based algorithm, which works by updating<br />candidates for centroids to be the mean of the points within a given<br />region. These candidates are then filtered in a post-processing stage to<br />eliminate near-duplicates to form the final set of centroids.<br />Seeding is performed using a binning technique for scalability.<br />Read more in the User Guide.<br />","url":"http://scikit-learn.org/stable/modules/generated/sklearn.cluster.MeanShift.html#sklearn.cluster.MeanShift","params":[{"name":"bandwidth","type":"float, optional","desc":"Bandwidth used in the RBF kernel. If not given, the bandwidth is estimated using sklearn.cluster.estimate_bandwidth; see the documentation for that function for hints on scalability (see also the Notes, below)."},{"name":"seeds","type":"array, shape=[n_samples, n_features], optional","desc":"Seeds used to initialize kernels. If not set, the seeds are calculated by clustering.get_bin_seeds with bandwidth as the grid size and default values for other parameters."},{"name":"bin_seeding","type":"boolean, optional","desc":"If true, initial kernel locations are not locations of all points, but rather the location of the discretized version of points, where points are binned onto a grid whose coarseness corresponds to the bandwidth. Setting this option to True will speed up the algorithm because fewer seeds will be initialized. default value: False Ignored if seeds argument is not None."},{"name":"min_bin_freq","type":"int, optional","desc":"To speed up the algorithm, accept only those bins with at least min_bin_freq points as seeds. If not defined, set to 1."},{"name":"cluster_all","type":"boolean, default True","desc":"If true, then all points are clustered, even those orphans that are not within any kernel. Orphans are assigned to the nearest kernel. If false, then orphans are given cluster label -1."},{"name":"n_jobs","type":"int","desc":"The number of jobs to use for the computation. This works by computing each of the n_init runs in parallel. If -1 all CPUs are used. If 1 is given, no parallel computing code is used at all, which is useful for debugging. For n_jobs below -1, (n_cpus + 1 + n_jobs) are used. Thus for n_jobs = -2, all CPUs but one are used."}],"attrs":[{"name":"cluster_centers_","type":"array, [n_clusters, n_features]","desc":"Coordinates of cluster centers."},{"name":"labels_","type":"","desc":"Labels of each point."}]},{"method":"Spectral Clustering","desc":"Apply clustering to a projection to the normalized laplacian.<br />In practice Spectral Clustering is very useful when the structure of<br />the individual clusters is highly non-convex or more generally when<br />a measure of the center and spread of the cluster is not a suitable<br />description of the complete cluster. For instance when clusters are<br />nested circles on the 2D plan.<br />If affinity is the adjacency matrix of a graph, this method can be<br />used to find normalized graph cuts.<br />When calling fit, an affinity matrix is constructed using either<br />kernel function such the Gaussian (aka RBF) kernel of the euclidean<br />distanced d(X, X):<br />np.exp(-gamma * d(X,X) ** 2)<br />or a k-nearest neighbors connectivity matrix.<br />Alternatively, using precomputed, a user-provided affinity<br />matrix can be used.<br />Read more in the User Guide.<br />","url":"http://scikit-learn.org/stable/modules/generated/sklearn.cluster.SpectralClustering.html#sklearn.cluster.SpectralClustering","params":[{"name":"n_clusters","type":"integer, optional","desc":"The dimension of the projection subspace."},{"name":"eigen_solver","type":"{None, ‘arpack’, ‘lobpcg’, or ‘amg’}","desc":"The eigenvalue decomposition strategy to use. AMG requires pyamg to be installed. It can be faster on very large, sparse problems, but may also lead to instabilities"},{"name":"random_state","type":"int, RandomState instance or None, optional, default","desc":"A pseudo random number generator used for the initialization of the lobpcg eigen vectors decomposition when eigen_solver == ‘amg’ and by the K-Means initialization.  If int, random_state is the seed used by the random number generator; If RandomState instance, random_state is the random number generator; If None, the random number generator is the RandomState instance used by np.random."},{"name":"n_init","type":"int, optional, default","desc":"Number of time the k-means algorithm will be run with different centroid seeds. The final results will be the best output of n_init consecutive runs in terms of inertia."},{"name":"gamma","type":"float, default=1.0","desc":"Kernel coefficient for rbf, poly, sigmoid, laplacian and chi2 kernels. Ignored for affinity='nearest_neighbors'."},{"name":"affinity","type":"string, array-like or callable, default ‘rbf’","desc":"If a string, this may be one of ‘nearest_neighbors’, ‘precomputed’, ‘rbf’ or one of the kernels supported by sklearn.metrics.pairwise_kernels. Only kernels that produce similarity scores (non-negative values that increase with similarity) should be used. This property is not checked by the clustering algorithm."},{"name":"n_neighbors","type":"integer","desc":"Number of neighbors to use when constructing the affinity matrix using the nearest neighbors method. Ignored for affinity='rbf'."},{"name":"eigen_tol","type":"float, optional, default","desc":"Stopping criterion for eigendecomposition of the Laplacian matrix when using arpack eigen_solver."},{"name":"assign_labels","type":"{‘kmeans’, ‘discretize’}, default","desc":"The strategy to use to assign labels in the embedding space. There are two ways to assign labels after the laplacian embedding. k-means can be applied and is a popular choice. But it can also be sensitive to initialization. Discretization is another approach which is less sensitive to random initialization."},{"name":"degree","type":"float, default=3","desc":"Degree of the polynomial kernel. Ignored by other kernels."},{"name":"coef0","type":"float, default=1","desc":"Zero coefficient for polynomial and sigmoid kernels. Ignored by other kernels."},{"name":"kernel_params","type":"dictionary of string to any, optional","desc":"Parameters (keyword arguments) and values for kernel passed as callable object. Ignored by other kernels."},{"name":"n_jobs","type":"int, optional (default = 1)","desc":"The number of parallel jobs to run. If -1, then the number of jobs is set to the number of CPU cores."}],"attrs":[{"name":"affinity_matrix_","type":"array-like, shape (n_samples, n_samples)","desc":"Affinity matrix used for clustering. Available only if after calling fit."},{"name":"labels_","type":"","desc":"Labels of each point"}]},{"method":"Spectral Biclustering","desc":"Spectral biclustering (Kluger, 2003).<br />Partitions rows and columns under the assumption that the data has<br />an underlying checkerboard structure. For instance, if there are<br />two row partitions and three column partitions, each row will<br />belong to three biclusters, and each column will belong to two<br />biclusters. The outer product of the corresponding row and column<br />label vectors gives this checkerboard structure.<br />Read more in the User Guide.<br />","url":"http://scikit-learn.org/stable/modules/generated/sklearn.cluster.bicluster.SpectralBiclustering.html#sklearn.cluster.bicluster.SpectralBiclustering","params":[{"name":"n_clusters","type":"integer or tuple (n_row_clusters, n_column_clusters)","desc":"The number of row and column clusters in the checkerboard structure."},{"name":"method","type":"string, optional, default","desc":"Method of normalizing and converting singular vectors into biclusters. May be one of ‘scale’, ‘bistochastic’, or ‘log’. The authors recommend using ‘log’. If the data is sparse, however, log normalization will not work, which is why the default is ‘bistochastic’. CAUTION: if method=’log’, the data must not be sparse."},{"name":"n_components","type":"integer, optional, default","desc":"Number of singular vectors to check."},{"name":"n_best","type":"integer, optional, default","desc":"Number of best singular vectors to which to project the data for clustering."},{"name":"svd_method","type":"string, optional, default","desc":"Selects the algorithm for finding singular vectors. May be ‘randomized’ or ‘arpack’. If ‘randomized’, uses sklearn.utils.extmath.randomized_svd, which may be faster for large matrices. If ‘arpack’, uses scipy.sparse.linalg.svds, which is more accurate, but possibly slower in some cases."},{"name":"n_svd_vecs","type":"int, optional, default","desc":"Number of vectors to use in calculating the SVD. Corresponds to ncv when svd_method=arpack and n_oversamples when svd_method is ‘randomized`."},{"name":"mini_batch","type":"bool, optional, default","desc":"Whether to use mini-batch k-means, which is faster but may get different results."},{"name":"init","type":"{‘k-means++’, ‘random’ or an ndarray}","desc":"Method for initialization of k-means algorithm; defaults to ‘k-means++’."},{"name":"n_init","type":"int, optional, default","desc":"Number of random initializations that are tried with the k-means algorithm. If mini-batch k-means is used, the best initialization is chosen and the algorithm runs once. Otherwise, the algorithm is run for each initialization and the best solution chosen."},{"name":"n_jobs","type":"int, optional, default","desc":"The number of jobs to use for the computation. This works by breaking down the pairwise matrix into n_jobs even slices and computing them in parallel. If -1 all CPUs are used. If 1 is given, no parallel computing code is used at all, which is useful for debugging. For n_jobs below -1, (n_cpus + 1 + n_jobs) are used. Thus for n_jobs = -2, all CPUs but one are used."},{"name":"random_state","type":"int, RandomState instance or None, optional, default","desc":"If int, random_state is the seed used by the random number generator; If RandomState instance, random_state is the random number generator; If None, the random number generator is the RandomState instance used by np.random."}],"attrs":[{"name":"rows_","type":"array-like, shape (n_row_clusters, n_rows)","desc":"Results of the clustering. rows[i, r] is True if cluster i contains row r. Available only after calling fit."},{"name":"columns_","type":"array-like, shape (n_column_clusters, n_columns)","desc":"Results of the clustering, like rows."},{"name":"row_labels_","type":"array-like, shape (n_rows,)","desc":"Row partition labels."},{"name":"column_labels_","type":"array-like, shape (n_cols,)","desc":"Column partition labels."}]},{"method":"Spectral Coclustering","desc":"Spectral Co-Clustering algorithm (Dhillon, 2001).<br />Clusters rows and columns of an array X to solve the relaxed<br />normalized cut of the bipartite graph created from X as follows:<br />the edge between row vertex i and column vertex j has weight<br />X[i, j].<br />The resulting bicluster structure is block-diagonal, since each<br />row and each column belongs to exactly one bicluster.<br />Supports sparse matrices, as long as they are nonnegative.<br />Read more in the User Guide.<br />","url":"http://scikit-learn.org/stable/modules/generated/sklearn.cluster.bicluster.SpectralCoclustering.html#sklearn.cluster.bicluster.SpectralCoclustering","params":[{"name":"n_clusters","type":"integer, optional, default","desc":"The number of biclusters to find."},{"name":"svd_method","type":"string, optional, default","desc":"Selects the algorithm for finding singular vectors. May be ‘randomized’ or ‘arpack’. If ‘randomized’, use sklearn.utils.extmath.randomized_svd, which may be faster for large matrices. If ‘arpack’, use scipy.sparse.linalg.svds, which is more accurate, but possibly slower in some cases."},{"name":"n_svd_vecs","type":"int, optional, default","desc":"Number of vectors to use in calculating the SVD. Corresponds to ncv when svd_method=arpack and n_oversamples when svd_method is ‘randomized`."},{"name":"mini_batch","type":"bool, optional, default","desc":"Whether to use mini-batch k-means, which is faster but may get different results."},{"name":"init","type":"{‘k-means++’, ‘random’ or an ndarray}","desc":"Method for initialization of k-means algorithm; defaults to ‘k-means++’."},{"name":"n_init","type":"int, optional, default","desc":"Number of random initializations that are tried with the k-means algorithm. If mini-batch k-means is used, the best initialization is chosen and the algorithm runs once. Otherwise, the algorithm is run for each initialization and the best solution chosen."},{"name":"n_jobs","type":"int, optional, default","desc":"The number of jobs to use for the computation. This works by breaking down the pairwise matrix into n_jobs even slices and computing them in parallel. If -1 all CPUs are used. If 1 is given, no parallel computing code is used at all, which is useful for debugging. For n_jobs below -1, (n_cpus + 1 + n_jobs) are used. Thus for n_jobs = -2, all CPUs but one are used."},{"name":"random_state","type":"int, RandomState instance or None, optional, default","desc":"If int, random_state is the seed used by the random number generator; If RandomState instance, random_state is the random number generator; If None, the random number generator is the RandomState instance used by np.random."}],"attrs":[{"name":"rows_","type":"array-like, shape (n_row_clusters, n_rows)","desc":"Results of the clustering. rows[i, r] is True if cluster i contains row r. Available only after calling fit."},{"name":"columns_","type":"array-like, shape (n_column_clusters, n_columns)","desc":"Results of the clustering, like rows."},{"name":"row_labels_","type":"array-like, shape (n_rows,)","desc":"The bicluster label of each row."},{"name":"column_labels_","type":"array-like, shape (n_cols,)","desc":"The bicluster label of each column."}]},{"method":"Isomap","desc":"Isomap Embedding<br />Non-linear dimensionality reduction through Isometric Mapping<br />Read more in the User Guide.<br />","url":"http://scikit-learn.org/stable/modules/generated/sklearn.manifold.Isomap.html#sklearn.manifold.Isomap","params":[{"name":"n_neighbors","type":"integer","desc":"number of neighbors to consider for each point."},{"name":"n_components","type":"integer","desc":"number of coordinates for the manifold"},{"name":"eigen_solver","type":"[‘auto’|’arpack’|’dense’]","desc":"‘auto’ : Attempt to choose the most efficient solver for the given problem. ‘arpack’ : Use Arnoldi decomposition to find the eigenvalues and eigenvectors. ‘dense’ : Use a direct solver (i.e. LAPACK) for the eigenvalue decomposition."},{"name":"tol","type":"float","desc":"Convergence tolerance passed to arpack or lobpcg. not used if eigen_solver == ‘dense’."},{"name":"max_iter","type":"integer","desc":"Maximum number of iterations for the arpack solver. not used if eigen_solver == ‘dense’."},{"name":"path_method","type":"string [‘auto’|’FW’|’D’]","desc":"Method to use in finding shortest path. ‘auto’ : attempt to choose the best algorithm automatically. ‘FW’ : Floyd-Warshall algorithm. ‘D’ : Dijkstra’s algorithm."},{"name":"neighbors_algorithm","type":"string [‘auto’|’brute’|’kd_tree’|’ball_tree’]","desc":"Algorithm to use for nearest neighbors search, passed to neighbors.NearestNeighbors instance."},{"name":"n_jobs","type":"int, optional (default = 1)","desc":"The number of parallel jobs to run. If -1, then the number of jobs is set to the number of CPU cores."}],"attrs":[{"name":"embedding_","type":"array-like, shape (n_samples, n_components)","desc":"Stores the embedding vectors."},{"name":"kernel_pca_","type":"object","desc":"KernelPCA object used to implement the embedding."},{"name":"training_data_","type":"array-like, shape (n_samples, n_features)","desc":"Stores the training data."},{"name":"nbrs_","type":"sklearn.neighbors.NearestNeighbors instance","desc":"Stores nearest neighbors instance, including BallTree or KDtree if applicable."},{"name":"dist_matrix_","type":"array-like, shape (n_samples, n_samples)","desc":"Stores the geodesic distance matrix of training data."}]},{"method":"Locally Linear Embedding","desc":"Locally Linear Embedding<br />Read more in the User Guide.<br />","url":"http://scikit-learn.org/stable/modules/generated/sklearn.manifold.LocallyLinearEmbedding.html#sklearn.manifold.LocallyLinearEmbedding","params":[{"name":"n_neighbors","type":"integer","desc":"number of neighbors to consider for each point."},{"name":"n_components","type":"integer","desc":"number of coordinates for the manifold"},{"name":"reg","type":"float","desc":"regularization constant, multiplies the trace of the local covariance matrix of the distances."},{"name":"eigen_solver","type":"string, {‘auto’, ‘arpack’, ‘dense’}","desc":"auto : algorithm will attempt to choose the best method for input data  arpack : use arnoldi iteration in shift-invert mode. For this method, M may be a dense matrix, sparse matrix, or general linear operator. Warning: ARPACK can be unstable for some problems.  It is best to try several random seeds in order to check results.  dense : use standard dense matrix operations for the eigenvalue decomposition.  For this method, M must be an array or matrix type.  This method should be avoided for large problems."},{"name":"tol","type":"float, optional","desc":"Tolerance for ‘arpack’ method Not used if eigen_solver==’dense’."},{"name":"max_iter","type":"integer","desc":"maximum number of iterations for the arpack solver. Not used if eigen_solver==’dense’."},{"name":"method","type":"string (‘standard’, ‘hessian’, ‘modified’ or ‘ltsa’)","desc":"standard : use the standard locally linear embedding algorithm.  see reference [1]  hessian : use the Hessian eigenmap method. This method requires n_neighbors > n_components * (1 + (n_components + 1) / 2 see reference [2]  modified : use the modified locally linear embedding algorithm. see reference [3]  ltsa : use local tangent space alignment algorithm see reference [4]"},{"name":"hessian_tol","type":"float, optional","desc":"Tolerance for Hessian eigenmapping method. Only used if method == 'hessian'"},{"name":"modified_tol","type":"float, optional","desc":"Tolerance for modified LLE method. Only used if method == 'modified'"},{"name":"neighbors_algorithm","type":"string [‘auto’|’brute’|’kd_tree’|’ball_tree’]","desc":"algorithm to use for nearest neighbors search, passed to neighbors.NearestNeighbors instance"},{"name":"random_state","type":"int, RandomState instance or None, optional (default=None)","desc":"If int, random_state is the seed used by the random number generator; If RandomState instance, random_state is the random number generator; If None, the random number generator is the RandomState instance used by np.random. Used when eigen_solver == ‘arpack’."},{"name":"n_jobs","type":"int, optional (default = 1)","desc":"The number of parallel jobs to run. If -1, then the number of jobs is set to the number of CPU cores."}],"attrs":[{"name":"embedding_vectors_","type":"array-like, shape [n_components, n_samples]","desc":"Stores the embedding vectors"},{"name":"reconstruction_error_","type":"float","desc":"Reconstruction error associated with embedding_vectors_"},{"name":"nbrs_","type":"NearestNeighbors object","desc":"Stores nearest neighbors instance, including BallTree or KDtree if applicable."}]},{"method":"MDS","desc":"Multidimensional scaling<br />Read more in the User Guide.<br />","url":"http://scikit-learn.org/stable/modules/generated/sklearn.manifold.MDS.html#sklearn.manifold.MDS","params":[{"name":"n_components","type":"int, optional, default","desc":"Number of dimensions in which to immerse the dissimilarities."},{"name":"metric","type":"boolean, optional, default","desc":"If True, perform metric MDS; otherwise, perform nonmetric MDS."},{"name":"n_init","type":"int, optional, default","desc":"Number of times the SMACOF algorithm will be run with different initializations. The final results will be the best output of the runs, determined by the run with the smallest final stress."},{"name":"max_iter","type":"int, optional, default","desc":"Maximum number of iterations of the SMACOF algorithm for a single run."},{"name":"verbose","type":"int, optional, default","desc":"Level of verbosity."},{"name":"eps","type":"float, optional, default","desc":"Relative tolerance with respect to stress at which to declare convergence."},{"name":"n_jobs","type":"int, optional, default","desc":"The number of jobs to use for the computation. If multiple initializations are used (n_init), each run of the algorithm is computed in parallel. If -1 all CPUs are used. If 1 is given, no parallel computing code is used at all, which is useful for debugging. For n_jobs below -1, (n_cpus + 1 + n_jobs) are used. Thus for n_jobs = -2, all CPUs but one are used."},{"name":"random_state","type":"int, RandomState instance or None, optional, default","desc":"The generator used to initialize the centers.  If int, random_state is the seed used by the random number generator; If RandomState instance, random_state is the random number generator; If None, the random number generator is the RandomState instance used by np.random."},{"name":"dissimilarity","type":"‘euclidean’ | ‘precomputed’, optional, default","desc":"Dissimilarity measure to use:   ‘euclidean’: Pairwise Euclidean distances between points in the dataset.    ‘precomputed’: Pre-computed dissimilarities are passed directly to fit and fit_transform."}],"attrs":[{"name":"embedding_","type":"array-like, shape (n_components, n_samples)","desc":"Stores the position of the dataset in the embedding space."},{"name":"stress_","type":"float","desc":"The final value of the stress (sum of squared distance of the disparities and the distances for all constrained points)."}]},{"method":"TSNE","desc":"t-distributed Stochastic Neighbor Embedding.<br />t-SNE [1] is a tool to visualize high-dimensional data. It converts<br />similarities between data points to joint probabilities and tries<br />to minimize the Kullback-Leibler divergence between the joint<br />probabilities of the low-dimensional embedding and the<br />high-dimensional data. t-SNE has a cost function that is not convex,<br />i.e. with different initializations we can get different results.<br />It is highly recommended to use another dimensionality reduction<br />method (e.g. PCA for dense data or TruncatedSVD for sparse data)<br />to reduce the number of dimensions to a reasonable amount (e.g. 50)<br />if the number of features is very high. This will suppress some<br />noise and speed up the computation of pairwise distances between<br />samples. For more tips see Laurens van der Maaten’s FAQ [2].<br />Read more in the User Guide.<br />","url":"http://scikit-learn.org/stable/modules/generated/sklearn.manifold.TSNE.html#sklearn.manifold.TSNE","params":[{"name":"n_components","type":"int, optional (default","desc":"Dimension of the embedded space."},{"name":"perplexity","type":"float, optional (default","desc":"The perplexity is related to the number of nearest neighbors that is used in other manifold learning algorithms. Larger datasets usually require a larger perplexity. Consider selecting a value between 5 and 50. The choice is not extremely critical since t-SNE is quite insensitive to this parameter."},{"name":"early_exaggeration","type":"float, optional (default","desc":"Controls how tight natural clusters in the original space are in the embedded space and how much space will be between them. For larger values, the space between natural clusters will be larger in the embedded space. Again, the choice of this parameter is not very critical. If the cost function increases during initial optimization, the early exaggeration factor or the learning rate might be too high."},{"name":"learning_rate","type":"float, optional (default","desc":"The learning rate for t-SNE is usually in the range [10.0, 1000.0]. If the learning rate is too high, the data may look like a ‘ball’ with any point approximately equidistant from its nearest neighbours. If the learning rate is too low, most points may look compressed in a dense cloud with few outliers. If the cost function gets stuck in a bad local minimum increasing the learning rate may help."},{"name":"n_iter","type":"int, optional (default","desc":"Maximum number of iterations for the optimization. Should be at least 250."},{"name":"n_iter_without_progress","type":"int, optional (default","desc":"Maximum number of iterations without progress before we abort the optimization, used after 250 initial iterations with early exaggeration. Note that progress is only checked every 50 iterations so this value is rounded to the next multiple of 50.  New in version 0.17: parameter n_iter_without_progress to control stopping criteria."},{"name":"min_grad_norm","type":"float, optional (default","desc":"If the gradient norm is below this threshold, the optimization will be stopped."},{"name":"metric","type":"string or callable, optional","desc":"The metric to use when calculating distance between instances in a feature array. If metric is a string, it must be one of the options allowed by scipy.spatial.distance.pdist for its metric parameter, or a metric listed in pairwise.PAIRWISE_DISTANCE_FUNCTIONS. If metric is “precomputed”, X is assumed to be a distance matrix. Alternatively, if metric is a callable function, it is called on each pair of instances (rows) and the resulting value recorded. The callable should take two arrays from X as input and return a value indicating the distance between them. The default is “euclidean” which is interpreted as squared euclidean distance."},{"name":"init","type":"string or numpy array, optional (default","desc":"Initialization of embedding. Possible options are ‘random’, ‘pca’, and a numpy array of shape (n_samples, n_components). PCA initialization cannot be used with precomputed distances and is usually more globally stable than random initialization."},{"name":"verbose","type":"int, optional (default","desc":"Verbosity level."},{"name":"random_state","type":"int, RandomState instance or None, optional (default","desc":"If int, random_state is the seed used by the random number generator; If RandomState instance, random_state is the random number generator; If None, the random number generator is the RandomState instance used by np.random.  Note that different initializations might result in different local minima of the cost function."},{"name":"method","type":"string (default","desc":"By default the gradient calculation algorithm uses Barnes-Hut approximation running in O(NlogN) time. method=’exact’ will run on the slower, but exact, algorithm in O(N^2) time. The exact algorithm should be used when nearest-neighbor errors need to be better than 3%. However, the exact method cannot scale to millions of examples.  New in version 0.17: Approximate optimization method via the Barnes-Hut."},{"name":"angle","type":"float (default","desc":"Only used if method=’barnes_hut’ This is the trade-off between speed and accuracy for Barnes-Hut T-SNE. ‘angle’ is the angular size (referred to as theta in [3]) of a distant node as measured from a point. If this size is below ‘angle’ then it is used as a summary node of all points contained within it. This method is not very sensitive to changes in this parameter in the range of 0.2 - 0.8. Angle less than 0.2 has quickly increasing computation time and angle greater 0.8 has quickly increasing error."}],"attrs":[{"name":"embedding_","type":"array-like, shape (n_samples, n_components)","desc":"Stores the embedding vectors."},{"name":"kl_divergence_","type":"float","desc":"Kullback-Leibler divergence after optimization."},{"name":"n_iter_","type":"int","desc":"Number of iterations run."}]},{"method":"Spectral Embedding","desc":"Spectral embedding for non-linear dimensionality reduction.<br />Forms an affinity matrix given by the specified function and<br />applies spectral decomposition to the corresponding graph laplacian.<br />The resulting transformation is given by the value of the<br />eigenvectors for each data point.<br />Note : Laplacian Eigenmaps is the actual algorithm implemented here.<br />Read more in the User Guide.<br />","url":"http://scikit-learn.org/stable/modules/generated/sklearn.manifold.SpectralEmbedding.html#sklearn.manifold.SpectralEmbedding","params":[{"name":"n_components","type":"integer, default","desc":"The dimension of the projected subspace."},{"name":"affinity","type":"string or callable, default","desc":"How to construct the affinity matrix.  ‘nearest_neighbors’ : construct affinity matrix by knn graph ‘rbf’ : construct affinity matrix by rbf kernel ‘precomputed’ : interpret X as precomputed affinity matrix callable : use passed in function as affinity the function takes in data matrix (n_samples, n_features) and return affinity matrix (n_samples, n_samples)."},{"name":"gamma","type":"float, optional, default","desc":"Kernel coefficient for rbf kernel."},{"name":"random_state","type":"int, RandomState instance or None, optional, default","desc":"A pseudo random number generator used for the initialization of the lobpcg eigenvectors.  If int, random_state is the seed used by the random number generator; If RandomState instance, random_state is the random number generator; If None, the random number generator is the RandomState instance used by np.random. Used when solver == ‘amg’."},{"name":"eigen_solver","type":"{None, ‘arpack’, ‘lobpcg’, or ‘amg’}","desc":"The eigenvalue decomposition strategy to use. AMG requires pyamg to be installed. It can be faster on very large, sparse problems, but may also lead to instabilities."},{"name":"n_neighbors","type":"int, default","desc":"Number of nearest neighbors for nearest_neighbors graph building."},{"name":"n_jobs","type":"int, optional (default = 1)","desc":"The number of parallel jobs to run. If -1, then the number of jobs is set to the number of CPU cores."}],"attrs":[{"name":"embedding_","type":"array, shape = (n_samples, n_components)","desc":"Spectral embedding of the training matrix."},{"name":"affinity_matrix_","type":"array, shape = (n_samples, n_samples)","desc":"Affinity_matrix constructed from samples or precomputed."}]},{"method":"Locally Linear Embedding","desc":"Perform a Locally Linear Embedding analysis on the data.<br />Read more in the User Guide.<br />","url":"http://scikit-learn.org/stable/modules/generated/sklearn.manifold.locally_linear_embedding.html#sklearn.manifold.locally_linear_embedding","params":[{"name":"X","type":"{array-like, sparse matrix, BallTree, KDTree, NearestNeighbors}","desc":"Sample data, shape = (n_samples, n_features), in the form of a numpy array, sparse array, precomputed tree, or NearestNeighbors object."},{"name":"n_neighbors","type":"integer","desc":"number of neighbors to consider for each point."},{"name":"n_components","type":"integer","desc":"number of coordinates for the manifold."},{"name":"reg","type":"float","desc":"regularization constant, multiplies the trace of the local covariance matrix of the distances."},{"name":"eigen_solver","type":"string, {‘auto’, ‘arpack’, ‘dense’}","desc":"auto : algorithm will attempt to choose the best method for input data  arpack : use arnoldi iteration in shift-invert mode. For this method, M may be a dense matrix, sparse matrix, or general linear operator. Warning: ARPACK can be unstable for some problems.  It is best to try several random seeds in order to check results.  dense : use standard dense matrix operations for the eigenvalue decomposition.  For this method, M must be an array or matrix type.  This method should be avoided for large problems."},{"name":"tol","type":"float, optional","desc":"Tolerance for ‘arpack’ method Not used if eigen_solver==’dense’."},{"name":"max_iter","type":"integer","desc":"maximum number of iterations for the arpack solver."},{"name":"method","type":"{‘standard’, ‘hessian’, ‘modified’, ‘ltsa’}","desc":"standard : use the standard locally linear embedding algorithm. see reference [R193]  hessian : use the Hessian eigenmap method.  This method requires n_neighbors > n_components * (1 + (n_components + 1) / 2. see reference [R194]  modified : use the modified locally linear embedding algorithm. see reference [R195]  ltsa : use local tangent space alignment algorithm see reference [R196]"},{"name":"hessian_tol","type":"float, optional","desc":"Tolerance for Hessian eigenmapping method. Only used if method == ‘hessian’"},{"name":"modified_tol","type":"float, optional","desc":"Tolerance for modified LLE method. Only used if method == ‘modified’"},{"name":"random_state","type":"int, RandomState instance or None, optional (default=None)","desc":"If int, random_state is the seed used by the random number generator; If RandomState instance, random_state is the random number generator; If None, the random number generator is the RandomState instance used by np.random. Used when solver == ‘arpack’."},{"name":"n_jobs","type":"int, optional (default = 1)","desc":"The number of parallel jobs to run for neighbors search. If -1, then the number of jobs is set to the number of CPU cores."}],"attrs":[{"name":"Y","type":"array-like, shape [n_samples, n_components]","desc":"Embedding vectors."},{"name":"squared_error","type":"float","desc":"Reconstruction error for the embedding vectors. Equivalent to norm(Y - W Y, 'fro')**2, where W are the reconstruction weights."}]},{"method":"Smacof","desc":"Computes multidimensional scaling using the SMACOF algorithm.<br />The SMACOF (Scaling by MAjorizing a COmplicated Function) algorithm is a<br />multidimensional scaling algorithm which minimizes an objective function<br />(the stress) using a majorization technique. Stress majorization, also<br />known as the Guttman Transform, guarantees a monotone convergence of<br />stress, and is more powerful than traditional techniques such as gradient<br />descent.<br />The SMACOF algorithm for metric MDS can summarized by the following steps:<br />Set an initial start configuration, randomly or not.<br />Compute the stress<br />Compute the Guttman Transform<br />Iterate 2 and 3 until convergence.<br />The nonmetric algorithm adds a monotonic regression step before computing<br />the stress.<br />","url":"http://scikit-learn.org/stable/modules/generated/sklearn.manifold.smacof.html#sklearn.manifold.smacof","params":[{"name":"dissimilarities","type":"ndarray, shape (n_samples, n_samples)","desc":"Pairwise dissimilarities between the points. Must be symmetric."},{"name":"metric","type":"boolean, optional, default","desc":"Compute metric or nonmetric SMACOF algorithm."},{"name":"n_components","type":"int, optional, default","desc":"Number of dimensions in which to immerse the dissimilarities. If an init array is provided, this option is overridden and the shape of init is used to determine the dimensionality of the embedding space."},{"name":"init","type":"ndarray, shape (n_samples, n_components), optional, default","desc":"Starting configuration of the embedding to initialize the algorithm. By default, the algorithm is initialized with a randomly chosen array."},{"name":"n_init","type":"int, optional, default","desc":"Number of times the SMACOF algorithm will be run with different initializations. The final results will be the best output of the runs, determined by the run with the smallest final stress. If init is provided, this option is overridden and a single run is performed."},{"name":"n_jobs","type":"int, optional, default","desc":"The number of jobs to use for the computation. If multiple initializations are used (n_init), each run of the algorithm is computed in parallel. If -1 all CPUs are used. If 1 is given, no parallel computing code is used at all, which is useful for debugging. For n_jobs below -1, (n_cpus + 1 + n_jobs) are used. Thus for n_jobs = -2, all CPUs but one are used."},{"name":"max_iter","type":"int, optional, default","desc":"Maximum number of iterations of the SMACOF algorithm for a single run."},{"name":"verbose","type":"int, optional, default","desc":"Level of verbosity."},{"name":"eps","type":"float, optional, default","desc":"Relative tolerance with respect to stress at which to declare convergence."},{"name":"random_state","type":"int, RandomState instance or None, optional, default","desc":"The generator used to initialize the centers.  If int, random_state is the seed used by the random number generator; If RandomState instance, random_state is the random number generator; If None, the random number generator is the RandomState instance used by np.random."},{"name":"return_n_iter","type":"bool, optional, default","desc":"Whether or not to return the number of iterations."}],"attrs":[{"name":"X","type":"ndarray, shape (n_samples, n_components)","desc":"Coordinates of the points in a n_components-space."},{"name":"stress","type":"float","desc":"The final value of the stress (sum of squared distance of the disparities and the distances for all constrained points)."},{"name":"n_iter","type":"int","desc":"The number of iterations corresponding to the best stress. Returned only if return_n_iter is set to True."}]},{"method":"Spectral Embedding","desc":"Project the sample on the first eigenvectors of the graph Laplacian.<br />The adjacency matrix is used to compute a normalized graph Laplacian<br />whose spectrum (especially the eigenvectors associated to the<br />smallest eigenvalues) has an interpretation in terms of minimal<br />number of cuts necessary to split the graph into comparably sized<br />components.<br />This embedding can also ‘work’ even if the adjacency variable is<br />not strictly the adjacency matrix of a graph but more generally<br />an affinity or similarity matrix between samples (for instance the<br />heat kernel of a euclidean distance matrix or a k-NN matrix).<br />However care must taken to always make the affinity matrix symmetric<br />so that the eigenvector decomposition works as expected.<br />Note : Laplacian Eigenmaps is the actual algorithm implemented here.<br />Read more in the User Guide.<br />","url":"http://scikit-learn.org/stable/modules/generated/sklearn.manifold.spectral_embedding.html#sklearn.manifold.spectral_embedding","params":[{"name":"adjacency","type":"array-like or sparse matrix, shape","desc":"The adjacency matrix of the graph to embed."},{"name":"n_components","type":"integer, optional, default 8","desc":"The dimension of the projection subspace."},{"name":"eigen_solver","type":"{None, ‘arpack’, ‘lobpcg’, or ‘amg’}, default None","desc":"The eigenvalue decomposition strategy to use. AMG requires pyamg to be installed. It can be faster on very large, sparse problems, but may also lead to instabilities."},{"name":"random_state","type":"int, RandomState instance or None, optional, default","desc":"A pseudo random number generator used for the initialization of the lobpcg eigenvectors decomposition.  If int, random_state is the seed used by the random number generator; If RandomState instance, random_state is the random number generator; If None, the random number generator is the RandomState instance used by np.random. Used when solver == ‘amg’."},{"name":"eigen_tol","type":"float, optional, default=0.0","desc":"Stopping criterion for eigendecomposition of the Laplacian matrix when using arpack eigen_solver."},{"name":"norm_laplacian","type":"bool, optional, default=True","desc":"If True, then compute normalized Laplacian."},{"name":"drop_first","type":"bool, optional, default=True","desc":"Whether to drop the first eigenvector. For spectral embedding, this should be True as the first eigenvector should be constant vector for connected graph, but for spectral clustering, this should be kept as False to retain the first eigenvector."}],"attrs":[{"name":"embedding","type":"array, shape=(n_samples, n_components)","desc":"The reduced samples."}]}]